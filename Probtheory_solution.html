<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Seoncheol Park" />


<title>Probability Theory</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<link href="site_libs/tutorial-0.4.0.9000/css/unilur.css" rel="stylesheet" />
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Gitbooks
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Probtheory_solution.html">Probability</a>
    </li>
    <li>
      <a href="https://seoncheolpark.github.io/book/_book/">통계공부와 관련된 글들</a>
    </li>
  </ul>
</li>
<li>
  <a href="links.html">Links</a>
</li>
<li>
  <a href="https://github.com/SeoncheolPark">
    <span class="fab fa fab fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Probability Theory</h1>
<h4 class="author">Seoncheol Park</h4>
<h4 class="date">06 September, 2020</h4>

</div>


<div id="measure-theory" class="section level1">
<h1><span class="header-section-number">1</span> Measure Theory</h1>
<div id="probability-spaces" class="section level2">
<h2><span class="header-section-number">1.1</span> Probability Spaces</h2>
<div class="definition" btit="probability space">
<p>A <strong>probability space</strong> is a triple <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>, where</p>
<ul>
<li><p><span class="math inline">\(\Omega\)</span> is a set of “outcomes,”</p></li>
<li><p><span class="math inline">\(\mathcal{F}\)</span> is a set of “events,” and</p></li>
<li><p><span class="math inline">\(P: \mathcal{F} \rightarrow [0,1]\)</span> is a function that assigns probabilities to events.</p></li>
</ul>
</div>
<div class="definition" btit="semialgebra">
<p>(Page 3 of <span class="citation">Durrett (2019)</span>)</p>
<p>A collction <span class="math inline">\(\mathcal{S}\)</span> of sets is said to be a <strong>semialgebra</strong> if</p>
<p>(i). it is closed under intersection, i.e., <span class="math inline">\(S, T \in \mathcal{S}\)</span> implies <span class="math inline">\(S \cap T \in \mathcal{S}\)</span>, and</p>
<p>(ii). if <span class="math inline">\(S \in\mathcal{S}\)</span>, then <span class="math inline">\(S^{c}\)</span> is a finite disjoint union of sets in <span class="math inline">\(\mathcal{S}\)</span>.</p>
</div>
<div class="definition" btit="field (algebra)">
<p>(Page 3 of <span class="citation">Durrett (2019)</span>)</p>
<p>A collction <span class="math inline">\(\mathcal{A}\)</span> of subsets of <span class="math inline">\(\Omega\)</span> is called an <strong>algebra</strong> (or <strong>field</strong>) if <span class="math inline">\(A,B \in \mathcal{A}\)</span> implies <span class="math inline">\(A^{c}\)</span> and <span class="math inline">\(A\cup B\)</span> are in <span class="math inline">\(\mathcal{A}\)</span>.</p>
</div>
<div class="definition" btit="σ -field">
<p>We assume that <span class="math inline">\(\mathcal{F}\)</span> is a <span class="math inline">\(\sigma\)</span>-<strong>field</strong> (or <span class="math inline">\(\sigma\)</span>-<strong>algebra</strong>), i.e., a (nonempty) collections of subsets of <span class="math inline">\(\Omega\)</span> that satisfy</p>
<p>(i). if <span class="math inline">\(A \in \mathcal{F}\)</span>, then <span class="math inline">\(A^{c} \in \mathcal{F}\)</span>, and</p>
<p>(ii). if <span class="math inline">\(A_{i} \in \mathcal{F}\)</span> is a countable sequence of sets, then <span class="math inline">\(\cup_{i} A_{i} \in \mathcal{F}\)</span>.</p>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-3">Question</a>
</h4>
</div>
<div id="unnamed-chunk-3" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2008-Summer
</div>
State the Borel <span class="math inline">\(\sigma\)</span>-field.
</div>
</div>
</div>
</div>
<p>By defintion, <span class="math inline">\(\sigma\)</span>-field is a field. But the converse is not true.</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-4">Question</a>
</h4>
</div>
<div id="unnamed-chunk-4" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="kaist">
2012-Winter
</div>
Let <span class="math inline">\((\Omega, \mathcal{B}, P)\)</span> be a probability space, and let <span class="math display">\[
\mathcal{G}:=\{A\in\mathcal{B}: P\{A\} =0\text{ or } 1 \}.
\]</span> Then, show that <span class="math inline">\(\mathcal{G}\)</span> is a <span class="math inline">\(\sigma\)</span>-field.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-5">Question</a>
</h4>
</div>
<div id="unnamed-chunk-5" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="kaist">
2016-Summer
</div>
Let <span class="math inline">\(\Omega = \{-1,0,1,2\}\)</span> and a function <span class="math inline">\(X\)</span> on <span class="math inline">\(\Omega\)</span> be defined by <span class="math inline">\(X(\omega) = \omega^2\)</span> for <span class="math inline">\(\omega \in \Omega\)</span>. Find the smallest <span class="math inline">\(\sigma\)</span>-field <span class="math inline">\(\sigma(X)\)</span> that makes <span class="math inline">\(X\)</span> a rnadom variable.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-6">Question</a>
</h4>
</div>
<div id="unnamed-chunk-6" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2019-Summer
</div>
Prove or disprove the following statment: For any sequence of <span class="math inline">\(\sigma\)</span>-fields <span class="math inline">\(\{\mathcal{F}_{n}\}_{n=1}^{\infty}, \bigcap_{n=1}^{\infty}\mathcal{F}_{n}\)</span> is a <span class="math inline">\(\sigma\)</span>-field.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-7">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-7" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p><a href="http://theanalysisofdata.com/probability/E_1.html">reference</a></p>
<ul>
<li><p>Since each <span class="math inline">\(\sigma\)</span>-field contains <span class="math inline">\(\Omega\)</span> their intersection is non-empty and it contains <span class="math inline">\(\Omega\)</span> as well.</p></li>
<li><p>If <span class="math inline">\(A\)</span> is a member of <span class="math inline">\(\bigcap_{n=1}^{\infty}\mathcal{F}_{n}\)</span>, then it is a member of all the <span class="math inline">\(\sigma\)</span>-fields and therefore <span class="math inline">\(A^{c}\)</span> is also a member of all the <span class="math inline">\(\sigma\)</span>-fields. It follows that <span class="math inline">\(A^{c}\)</span> is also in the intersection.</p></li>
<li><p>The intersections of the <span class="math inline">\(\sigma\)</span>-fields is closed under countable unions and intersections for the same reaseon.</p></li>
</ul>
</div>
</div>
</div>
</div>
<p>Without <span class="math inline">\(P\)</span>, <span class="math inline">\((\Omega, \mathcal{F})\)</span> is called a <strong>measurable space</strong>, i.e., it is a space on which we can put a measure.</p>
<div class="definition" btit="measure">
<p>A <strong>measure</strong> is a nonnegative countably additive set function; that is, a function <span class="math inline">\(\mu: \mathcal{F} \rightarrow \mathbb{R}\)</span> with</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mu(A) \geq \mu (\emptyset) = 0\)</span> for all <span class="math inline">\(A \in \mathcal{F}\)</span>, and</p></li>
<li><p>if <span class="math inline">\(A_{i} \in \mathcal{F}\)</span> is a countable sequence of disjoint sets, then <span class="math display">\[
\mu (\cup_{i}A_{i}) = \sum_{i}\mu(A_{i}).
\]</span></p></li>
</ol>
</div>
<p>If <span class="math inline">\(\mu(\Omega) = 1\)</span>, we call <span class="math inline">\(\mu\)</span> a <strong>probability measure</strong>.</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-8">Question</a>
</h4>
</div>
<div id="unnamed-chunk-8" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2007-Summer
</div>
<div class="snu">
2008-Summer
</div>
State the probability measure.
</div>
</div>
</div>
</div>
<div class="theorem" btit="properties of a measure">
<p>Let <span class="math inline">\(\mu\)</span> be a measure on <span class="math inline">\((\Omega, \mathcal{F})\)</span>.</p>
<p>(i). <strong>monotonicity</strong>: If <span class="math inline">\(A\subset B\)</span>, then <span class="math inline">\(\mu(A) \leq \mu(B)\)</span>.</p>
<p>(ii). <strong>subadditivity</strong>: If <span class="math inline">\(A\subset \cup_{m=1}^{\infty}A_m\)</span>, then <span class="math inline">\(\mu(A) \leq \sum_{m=1}^{\infty}\mu(A_m)\)</span>.</p>
<p>(iii). <strong>continuity from below</strong>: If <span class="math inline">\(A_i \uparrow A\)</span> (i.e., <span class="math inline">\(A_1 \subset A_2 \subset \ldots\)</span> and <span class="math inline">\(\cup_{i}A_i = A\)</span>), then <span class="math inline">\(\mu(A_i) \uparrow \mu(A)\)</span>.</p>
<p>(iv). <strong>continuity from above</strong>: If <span class="math inline">\(A_i \downarrow A\)</span> (i.e., <span class="math inline">\(A_1 \supset A_2 \supset \ldots\)</span> and <span class="math inline">\(\cap_{i}A_i = A\)</span>), with <span class="math inline">\(\mu(A_1) &lt;\infty\)</span>, then <span class="math inline">\(\mu(A_i) \downarrow \mu(A)\)</span>.</p>
</div>
<div class="proof">
<p>Page 2 of <span class="citation">Durrett (2019)</span> .</p>
</div>
<p>If <span class="math inline">\(\mathcal{F}_i, i\in I\)</span> are <span class="math inline">\(\sigma\)</span>-fields, then <span class="math inline">\(\cap_{i\in I} \mathcal{F}_{i}\)</span> is. Here <span class="math inline">\(I\neq \emptyset\)</span> is an arbitrary index set (i.e., possibily uncountable) From this it follows that if we are given a set <span class="math inline">\(\Omega\)</span> and a collection <span class="math inline">\(\mathcal{A}\)</span> of subsets of <span class="math inline">\(\Omega\)</span>, then there is a smallest <span class="math inline">\(\sigma\)</span>-field containing <span class="math inline">\(\mathcal{A}\)</span>. We will call this the <span class="math inline">\(\sigma\)</span>-<strong>field generated by</strong> <span class="math inline">\(\mathcal{A}\)</span> and denote it by <span class="math inline">\(\sigma(\mathcal{A})\)</span>.</p>
<p>Let <span class="math inline">\(\mathbb{R}^{d}\)</span> be the set of vectors <span class="math inline">\((x_i, \ldots, x_d)\)</span> of real numbers and <span class="math inline">\(\mathcal{R}^{d}\)</span> be the <strong>Borel sets</strong>, the smallest <span class="math inline">\(\sigma\)</span>-field containing the open sets.</p>
<div class="example" btit="measures on the real line">
<p>Measures on <span class="math inline">\((\mathbb{R}, \mathcal{R})\)</span> are defined by giving a <strong>Stieltjes measure function</strong> with the following properties:</p>
<p>(i). <span class="math inline">\(F\)</span> is nondecreasing.</p>
<p>(ii). <span class="math inline">\(F\)</span> is right continuous, i.e., <span class="math inline">\(\lim_{y\downarrow x}F(y) = F(x)\)</span>.</p>
</div>
<div class="theorem" btit="Lebesgue measure">
<p>Associated with each Stieltjes measure function <span class="math inline">\(F\)</span> there is a unique measure <span class="math inline">\(\mu\)</span> on <span class="math inline">\((\mathbb{R}, \mathcal{R})\)</span> with <span class="math display">\[\mu((a,b]) = F(b)- F(a).\]</span> When <span class="math inline">\(F(x) = x\)</span>, the resulting measure is called <strong>Lebesgue measure</strong>.</p>
</div>
<div class="lemma" btit="algebra generated by semialgebra">
<p>If <span class="math inline">\(\mathcal{S}\)</span> is a semialgebra, then <span class="math inline">\(\bar{\mathcal{S}} = \{\text{finite disjoint unions of sets in } \mathcal{S} \}\)</span> is an algebra, called the <strong>algebra generated by</strong> <span class="math inline">\(\mathcal{S}\)</span>.</p>
</div>
<div class="proof">
<p>(Page 3 of <span class="citation">Durrett (2019)</span>)</p>
<p>Suppose <span class="math inline">\(A = +_{i}S_i\)</span> and <span class="math inline">\(B = +_{j}T_{j}\)</span>, where <span class="math inline">\(+\)</span> denotes <strong>disjoint union</strong> and we assum the index sets are finite. Then <span class="math inline">\(A \cap B = +_{i,j}S_{i}\cap T_{j} \in \bar{\mathcal{S}}\)</span>. As for complements, if <span class="math inline">\(A= +_{i}S_i\)</span> then <span class="math inline">\(A^{c} = \cap_{i}S_{i}^{c}\)</span>. The definition of <span class="math inline">\(\mathcal{S}\)</span> implies <span class="math inline">\(S_{i}^{c} \in \bar{\mathcal{S}}\)</span>. We have shown that <span class="math inline">\(\bar{\mathcal{S}}\)</span> is closed under intersetions, so it follows by induction that <span class="math inline">\(A^{c} \in \bar{\mathcal{S}}\)</span>.</p>
</div>
<div class="example" btit="measure on algebra">
<p>Let <span class="math inline">\(\Omega = \mathbb{R}\)</span> and <span class="math inline">\(\mathcal{S} = \mathcal{S}_1\)</span> then <span class="math inline">\(\bar{\mathcal{S}}_{1}\)</span>=the empty set plus all sets of the form <span class="math display">\[
\cup_{i=1}^{k}(a_i, b_i], \quad{\text{where } -\infty \leq a_i &lt; b_i \leq \infty}
\]</span> Given a set function <span class="math inline">\(\mu\)</span> on <span class="math inline">\(\mathcal{S}\)</span> we can extend it to <span class="math inline">\(\bar{\mathcal{S}}\)</span> by <span class="math display">\[
\mu(+_{i=1}^{n}A_i) = \sum_{i=1}^{n}\mu(A_i).
\]</span></p>
<p>By a <strong>measure on an algebra</strong> <span class="math inline">\(\mathcal{A}\)</span>, we mean a set function <span class="math inline">\(\mu\)</span> with</p>
<p>(i). <span class="math inline">\(\mu(A)\geq \mu(\emptyset)\)</span> for all <span class="math inline">\(A \in \mathcal{A}\)</span>, and</p>
<p>(ii). if <span class="math inline">\(A_i \in \mathcal{A}\)</span> are disjoint <em>and their union is in</em> <span class="math inline">\(\mathcal{A}\)</span>, then <span class="math display">\[
\mu(\cup_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty}\mu(A_i).
\]</span> <span class="math inline">\(\mu\)</span> is said to be <span class="math inline">\(\sigma\)</span>-<strong>finite</strong> if there is a sequence of sets <span class="math inline">\(A_n \in \mathcal{A}\)</span> so that <span class="math inline">\(\mu(A_n) &lt;\infty\)</span> and <span class="math inline">\(\cup_{n}A_n =\Omega\)</span>. Letting <span class="math inline">\(A_{1}&#39;=A_1\)</span> and for <span class="math inline">\(n\geq 2\)</span>, <span class="math display">\[
A_{n}&#39; = \cup_{m=1}^{n}A_m \quad{\text{or } A_n&#39; = A_n \cap \big( \cap_{m=1}^{n-1}A_m^{c} \big)} \in \mathcal{A}.
\]</span> We can without loss of generality assume that <span class="math inline">\(A_n \uparrow \Omega\)</span> or the <span class="math inline">\(A_n\)</span> are disjoint.</p>
</div>
</div>
<div id="dynkins-system-theorem" class="section level2">
<h2><span class="header-section-number">1.2</span> Dynkin’s System Theorem</h2>
<p>It is often the case that two measures which agree on a certain class of sets actually agree on all sets in the relevant <span class="math inline">\(\sigma\)</span>-algebra. There are a couple of standard tools to prove that the measures are the same:</p>
<ol style="list-style-type: decimal">
<li><p>the Monotone Class lemma and</p></li>
<li><p>the Dynkin <span class="math inline">\(\pi - \lambda\)</span> theorem.</p></li>
</ol>
<p>They are essentially equivalent devices and it is largely a matter of taste which one to take as standard equipment.</p>
<p>For more about the motivation of Dynkin’s theorem, please check <a href="https://math.stackexchange.com/questions/1769131/what-is-a-dynkin-system-lambda-system/1769181">this</a> .</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-9">Question</a>
</h4>
</div>
<div id="unnamed-chunk-9" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
1999-Summer
</div>
<div class="snu">
2003-Summer
</div>
<div class="snu">
2004-Winter
</div>
<div class="snu">
2004-Summer
</div>
<div class="snu">
2007-Summer
</div>
<div class="snu">
2008-Summer
</div>
<div class="snu">
2009-Summer
</div>
<div class="snu">
2009-Winter
</div>
<div class="snu">
2010-Summer
</div>
<div class="snu">
2011-Summer
</div>
<div class="snu">
2011-Winter
</div>
<div class="snu">
2013-Winter
</div>
<div class="snu">
2014-Summer
</div>
<div class="snu">
2014-Winter
</div>
<div class="snu">
2016-Winter
</div>
<div class="snu">
2017-Summer
</div>
<div class="snu">
2017-Winter
</div>
<div class="snu">
2018-Summer
</div>
<div class="snu">
2018-Winter
</div>
<div class="kaist">
2018-Winter
</div>
State Dynkin’s <span class="math inline">\(\pi-\lambda\)</span> system theorem.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-10">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-10" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Durrett (2019)</span>, Theorem 2.1.6) We say that <span class="math inline">\(\mathcal{A}\)</span> is a</p>
<ul>
<li><p><span class="math inline">\(\pi\)</span>-<strong>system</strong> if it is closed under intersection, i.e., if <span class="math inline">\(A, B\in \mathcal{A}\)</span>, then <span class="math inline">\(A \cap B \in \mathcal{A}\)</span>, and</p></li>
<li><p><span class="math inline">\(\lambda\)</span>-<strong>system</strong> if (i) <span class="math inline">\(\Omega \in \mathcal{L}\)</span>, (ii) if <span class="math inline">\(A, B \in \mathcal{L}\)</span>, and <span class="math inline">\(A\subset B\)</span>, then <span class="math inline">\(B - A \in \mathcal{L}\)</span>, and (iii) if <span class="math inline">\(A_{n} \in \mathcal{L}\)</span> and <span class="math inline">\(A_{n} \uparrow A\)</span>, then <span class="math inline">\(A\in\mathcal{L}\)</span>.</p></li>
</ul>
<div class="theorem" btit="π - λ Theorem">
<p><span class="math inline">\(\pi - \lambda\)</span> Theorem: If <span class="math inline">\(\mathcal{P}\)</span> is a <span class="math inline">\(\pi\)</span>-system and <span class="math inline">\(\lambda\)</span> is a <span class="math inline">\(\lambda\)</span> system that contains <span class="math inline">\(\mathcal{P}\)</span>, then <span class="math inline">\(\sigma (\mathcal{P})\subset \mathcal{L}\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-11">Question</a>
</h4>
</div>
<div id="unnamed-chunk-11" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2019-Summer
</div>
Prove or disprove the following statment: Every <span class="math inline">\(\pi\)</span>-system is also a field (an algebra.)
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-12">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-12" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p><a href="http://users.jyu.fi/~geiss/lectures/probability-1.pdf">reference, page 24</a></p>
Any algebra is a <span class="math inline">\(\pi\)</span>-system but a <span class="math inline">\(\pi\)</span>-system is not an algebra in general, take for example the <span class="math inline">\(\pi\)</span>-system <span class="math inline">\(\{(a,b): -\infty &lt; a &lt; b &lt; \infty \} \cup \{\emptyset\}\)</span>.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-13">Question</a>
</h4>
</div>
<div id="unnamed-chunk-13" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2012-Summer
</div>
<div class="kaist">
2013-Summer
</div>
<div class="snu">
2015-Winter
</div>
<p>If for each <span class="math inline">\(i=1, \ldots, n\)</span>, <span class="math inline">\(\mathcal{C}_{i}\)</span> is a non-empty class of events satisfying</p>
<p>(a). <span class="math inline">\(\mathcal{C}_{i}\)</span> is a <span class="math inline">\(\pi\)</span>-system</p>
<p>(b). <span class="math inline">\(\mathcal{C}_{i}\)</span> <span class="math inline">\(i=1,\ldots, n\)</span> are independent,</p>
then show that <span class="math inline">\(\sigma(\mathcal{C}_{1}), \ldots , \sigma(\mathcal{C}_{n})\)</span> are independent.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-14">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-14" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Durrett (2019)</span>, Theorem 2.1.7)</p>
<p><a href="http://theanalysisofdata.com/probability/E_4.html">reference</a></p>
First, note that <span class="math inline">\(\mathcal{C}_{i}&#39; = \mathcal{C}_{i} \cup \{\Omega \}\)</span> are also <span class="math inline">\(\pi\)</span>-system. Fix <span class="math inline">\(C_{i}&#39; \in \mathcal{C}_{i}&#39;\)</span> for <span class="math inline">\(i=2,\ldots, n\)</span> and consider the set <span class="math inline">\(\mathcal{L}\)</span> of sets <span class="math inline">\(L\)</span> for which <span class="math display">\[
P(L \cap C_{2}&#39; \cap \cdots C_{n}&#39;) = P(L)P(C_{2}&#39;)\cdots P(C_{n}&#39;).
\]</span> We show next that <span class="math inline">\(\mathcal{L}\)</span> is a <span class="math inline">\(\lambda\)</span>-system. The first condition hholds since <span class="math display">\[
P(\Omega \cap C_{2}&#39; \cap \cdots \cap C_{n}&#39;) = 1 \cdot P(C_{2}&#39;)\cdots P(C_{n}&#39;) = P(\Omega) \cdot P(C_{2}&#39;)\cdots P(C_{n}&#39;).
\]</span> The second condition holds since <span class="math inline">\(U\in \mathcal{L}\)</span> implies <span class="math display">\[\begin{align}
P(U^{c} \cap C_{2}&#39; \cap \cdots \cap C_{n}&#39;) &amp;= P((C_{2}&#39; \cap \cdots \cap C_{n}&#39;)\backslash U)\\
&amp;= P(C_{2}&#39; \cap \cdots C_{n}&#39;) - P(C_{2}\cap \cdots \cap C_{n}&#39;\cap U)\\
&amp;= P(C_{2}&#39;)\cdots P(C_{n}&#39;) - P(C_{2})\cdots P(C_{n}&#39;)P(U)\\
&amp;= P(C_{2}&#39;)\cdots P(C_{n}&#39;)P(U^{c}).
\end{align}\]</span> The third property follows from the fact that for a disjoint sequence of sets <span class="math inline">\(U_n \in \mathcal{L}, n \in \mathbb{N}\)</span> implies <span class="math display">\[\begin{align}
P((\cup_{n}U_{n})\cap C_{2}&#39; \cap \cdots \cap C_{n}&#39;) &amp;= P(\cup_{n}(U_{n} \cap C_{2}&#39; \cap \cdots \cap C_{n}&#39;))\\
&amp;= \sum_{n}P(U_{n} \cap C_{2}&#39; \cap \cdots \cap C_{n}&#39;)\\
&amp;= \sum_{n}P(U_{n})P(C_{2}&#39;)\cdots P(C_{n}&#39;)\\
&amp;= P(C_{2}&#39;)\cdots P(C_{n}&#39;)P(\cup_{n}U_n).\\
\end{align}\]</span> Since <span class="math inline">\(\mathcal{L}\)</span> is a <span class="math inline">\(\lambda\)</span>-system that also contains the <span class="math inline">\(\pi\)</span>-system <span class="math inline">\(C_{1}&#39;\)</span>, by Dynkin’s theorem we have that <span class="math inline">\(\sigma(\mathcal{C}_{1}&#39;), \{C_{2}&#39;\}, \ldots, \{C_{n}&#39;\}\)</span> are independent. Since <span class="math inline">\(C_{i}&#39;\)</span> were selected arbitrary from <span class="math inline">\(\mathcal{C}_{i}&#39;\)</span>, it follows that <span class="math inline">\(\sigma(\mathcal{C}_{1}&#39;),\mathcal{C}_{2},\ldots,\mathcal{C}_{n}\)</span> are independent as well. Repeating this arguments multiple times we have that <span class="math inline">\(\sigma(\mathcal{C}_{1}&#39;),\sigma(\mathcal{C}_{2}&#39;),\ldots , \sigma(\mathcal{C}_{n})\)</span> are independent. (Maybe we need to show that <span class="math inline">\(\sigma(\mathcal{C}_{i}&#39;)=\sigma(\mathcal{C}_{i})\)</span> for each <span class="math inline">\(i\)</span>.)
</div>
</div>
</div>
</div>
<div id="an-approximation-lemma" class="section level3">
<h3><span class="header-section-number">1.2.1</span> An Approximation Lemma</h3>
<p>(Section 1.3.2 of <span class="citation">(Gut 2014)</span>)</p>
<p>The following result states that any set in a <span class="math inline">\(\sigma\)</span>-algebra can be arbitrary well approximated by another set that belongs to an algebra that generates the <span class="math inline">\(\sigma\)</span>-algebra. The need for this result is the fact that the infinite union of <span class="math inline">\(\sigma\)</span>-algebras is not necessarily a <span class="math inline">\(\sigma\)</span>-algebra.</p>
<p>The general description of the result reveals that it reduces an infinite setting to a finite one, which suggests that the proof builds on the <strong>metatheorem technique</strong>.</p>
<p>The following problem is an <strong>approximation lemma</strong>.</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-15">Question</a>
</h4>
</div>
<div id="unnamed-chunk-15" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
1999-Summer
</div>
Let <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> be a probability space and suppose that <span class="math inline">\(\mathcal{F}\)</span> is generated by a field <span class="math inline">\(\mathcal{F}_0\)</span> (that is, <span class="math inline">\(\mathcal{F} = \sigma(\mathcal{F}_{0})\)</span>). Show that for any <span class="math inline">\(A \in \mathcal{F}_0\)</span> &amp; <span class="math inline">\(\epsilon &gt;0\)</span>, there exists <span class="math inline">\(B \in \mathcal{F}_0\)</span>, such that <span class="math inline">\(P(A \Delta B) &lt; \epsilon\)</span>, where <span class="math inline">\(A \Delta B = (A-B) \cup (B-A)\)</span>.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-16">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-16" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Gut (2014)</span>, Lemma 1.3.1, page 14)</p>
<p>Let &gt;0, and define <span class="math display">\[
\mathcal{G} = \{ A \in \mathcal{F} : P(A \Delta B)&lt;\epsilon \quad{\text{for some }} B \in \mathcal{F}_{0}\}.
\]</span></p>
<p>(i). If <span class="math inline">\(A\in\mathcal{G}\)</span>, then <span class="math inline">\(A^{c}\in\mathcal{G}\)</span>, since <span class="math inline">\(A^{c} \Delta (B)^{c} = A \Delta B\)</span>.</p>
<p>(ii). If <span class="math inline">\(A_n \in \mathcal{G}, n\geq 1\)</span>, then so does the union. Namely, set <span class="math inline">\(A=\sup_{n=1}^{\infty}A_n\)</span>, let <span class="math inline">\(\epsilon\)</span> be given and choose <span class="math inline">\(n_{*}\)</span> such that <span class="math display">\[
P\Big(A \backslash \bigcup_{k=1}^{n_{*}}A_k \Big) &lt; \epsilon.
\]</span> Next, let <span class="math inline">\(\{ B_{k} \in \mathcal{F}_0 , 1\leq k \leq n_{*} \}\)</span> be such that <span class="math display">\[
P(A_{k}\Delta B_{k}) &lt; \epsilon \quad{\text{for } 1\leq k \leq n_{*}}.
\]</span> Since <span class="math display">\[
\Big(\bigcup_{k=1}^{n_{*}}A_k \Big) \Delta \Big(\bigcup_{k=1}^{n_{*}}B_k \Big) \subset \bigcup_{k=1}^{n_{*}}(A_k \Delta B_k),
\]</span> it follows that <span class="math display">\[
P\Bigg(\Big( \bigcup_{k=1}^{n_{*}}A_k \Big) \Delta \Big( \bigcup_{k=1}^{n_{*}}B_k \Big)  \Bigg) \leq \sum_{k=1}^{n_{*}}P(A_{k}\Delta B_k) &lt;n_{*}\epsilon,
\]</span> so that, finally, <span class="math display">\[
P\Bigg(A \Delta \Big( \bigcup_{k=1}^{n_{*}}B_k \Big)  \Bigg)  &lt; (n_{*}+1)\epsilon.
\]</span> This proves the second claim—the claim would have followed with an approximation error <span class="math inline">\(\epsilon\)</span> instead of <span class="math inline">\((n_{*}+1)\epsilon\)</span> if we had chosen <span class="math inline">\(\epsilon\)</span> to be <span class="math inline">\(\frac{\epsilon}{2}\)</span> in <span class="math inline">\(P\Big(A \backslash \bigcup_{k=1}^{n_{*}}A_k \Big) &lt; \epsilon\)</span> and as <span class="math inline">\(\frac{\epsilon}{2n_{*}}\)</span> in <span class="math inline">\(P(A_{k}\Delta B_{k}) &lt; \epsilon\)</span>. But that is cheating.</p>
To summarize: <span class="math inline">\(\mathcal{G}\)</span> is non-empty, since <span class="math inline">\(\mathcal{G} \supset \mathcal{F}_{0}\)</span> by construction (choose <span class="math inline">\(B=A\)</span> whenever <span class="math inline">\(A\in\mathcal{F}_{0}\)</span>), and <span class="math inline">\(\mathcal{G}\)</span> obeys properties of <span class="math inline">\(\sigma\)</span>-algebra, so that <span class="math inline">\(\mathcal{G}\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra. Moreover, <span class="math inline">\(\mathcal{G} \supset \mathcal{F}\)</span>, since <span class="math inline">\(\mathcal{F}\)</span> is the minimal <span class="math inline">\(\sigma\)</span>-algebra containing <span class="math inline">\(\mathcal{F}_{0}\)</span>, and, since, trivially, <span class="math inline">\(\mathcal{G} \subset \mathcal{F}\)</span>, it finally follows that <span class="math inline">\(\mathcal{G} = \mathcal{F}\)</span>.
</div>
</div>
</div>
</div>
</div>
</div>
<div id="distributions" class="section level2">
<h2><span class="header-section-number">1.3</span> Distributions</h2>
<p>Probability spaces become a little more interesting when we define random variables on them. A real-valued function</p>
<div class="definition" btit="random variable">
<p>A real-valued function <span class="math inline">\(X\)</span> defined on <span class="math inline">\(\Omega\)</span> is said to be a <strong>random variable</strong> if for every Borel set <span class="math inline">\(B \subset \mathbb{R}\)</span> we have <span class="math inline">\(X^{-1}(B) = \{\omega : X(\omega) \in B \} \in \mathcal{F}\)</span>.</p>
</div>
<p>When we need to emphasize the <span class="math inline">\(\sigma\)</span>-field, we will say that <span class="math inline">\(X\)</span> is <span class="math inline">\(\mathcal{F}\)</span>-<strong>measurable</strong> or write <span class="math inline">\(X \in \mathcal{F}\)</span>.</p>
<ul>
<li>If <span class="math inline">\(\Omega\)</span> is a discrete probability space, then any function <span class="math inline">\(X: \Omega \rightarrow \mathbb{R}\)</span> is a random variable.</li>
</ul>
<div class="definition" btit="distribution">
<p>If <span class="math inline">\(X\)</span> is a random variable, then <span class="math inline">\(X\)</span> induces a proability measure of <span class="math inline">\(\mathbb{R}\)</span> called its <strong>distribution</strong> by setting <span class="math inline">\(\mu(A) = P(X \in A)\)</span> for Borel sets <span class="math inline">\(A\)</span>. Using the notation introduced previously, the right-hand side can be written as <span class="math inline">\(P(X^{-1}(A))\)</span>. In words, we pul <span class="math inline">\(A \in \mathcal{R}\)</span> back to <span class="math inline">\(X^{-1}(A)\in \mathcal{F}\)</span> and then take <span class="math inline">\(P\)</span> of that set.</p>
</div>
<p>To check that <span class="math inline">\(\mu\)</span> is a probability measure we observe that if the <span class="math inline">\(A_i\)</span> are disjoint, then using the definition of <span class="math inline">\(\mu\)</span>; the fact that <span class="math inline">\(X\)</span> lands in the union if and only if it lands in one of the <span class="math inline">\(A_i\)</span>; the fact that if the sets <span class="math inline">\(A_i \in \mathcal{R}\)</span> are disjoint, then the events <span class="math inline">\(\{ X \in A_i \}\)</span> are disjoint; and the definition of <span class="math inline">\(\mu\)</span> again; we have: <span class="math display">\[
\mu (\cup_{i}A_i) = P(X\in\cup_{i}A_i) = P(\cup_{i}\{ X \in A_i \}) = \sum_{i} P(X \in A_i) = \sum_{i} \mu(A_i).
\]</span></p>
<div class="definition" btit="distribution function">
<p>The distribution of a random variable <span class="math inline">\(X\)</span> is usually described by giving its <strong>distribution function</strong>, <span class="math inline">\(F(x) = P(X\leq x)\)</span>.</p>
</div>
<div class="theorem" btit="properties of distribution function">
<p>(i). <span class="math inline">\(F\)</span> is nondecreasing.</p>
<p>(ii). <span class="math inline">\(\lim_{x\rightarrow \infty}F(x) = 1\)</span>, <span class="math inline">\(\lim_{x\rightarrow -\infty}F(x) =0\)</span>.</p>
<p>(iii). <span class="math inline">\(F\)</span> is right continuous, i.e., <span class="math inline">\(\lim_{y\downarrow x}F(y) = F(x)\)</span>.</p>
<p>(iv). If <span class="math inline">\(F(x-) = \lim_{y\uparrow x}F(y)\)</span>, then <span class="math inline">\(F(x-) = P(X&lt;x)\)</span>.</p>
<p>(v). $P(X = x) = F(x) - F(x-),</p>
</div>
<div class="theorem" btit="distribution function of some random variable">
<p>If <span class="math inline">\(F\)</span> satisfies (i), (ii), and (iii) in the previos theorem, then it is the distribution function of some random variable.</p>
</div>
<div class="proof">
<p>Let <span class="math inline">\(\Omega = (0,1)\)</span>, <span class="math inline">\(\mathcal{F}=\text{the Borel sets, and }\)</span> <span class="math inline">\(P=\text{Lebesgue measure}\)</span>. If <span class="math inline">\(\omega \in (0,1)\)</span>, let <span class="math display">\[
X(\omega) = \sup\{y:F(y)&lt;\omega\}.
\]</span> Once we show that <span class="math display">\[
\{\omega: X(\omega)\leq x\} = \{\omega: \omega \leq F(x) \}
\]</span> the desired result follows immediately since <span class="math inline">\(P(\omega: \omega \leq F(x)) = F(x)\)</span>. (Recall <span class="math inline">\(P\)</span> is Lebesgue measure.) To check <span class="math inline">\(\{\omega: X(\omega)\leq x\} = \{\omega: \omega \leq F(x) \}\)</span>, we observe that if <span class="math inline">\(\omega \leq F(x)\)</span>, then <span class="math inline">\(X(\omega) \leq x\)</span>, since <span class="math inline">\(x\notin \{y : F(y)&lt;\omega\}\)</span>. On the other hand if <span class="math inline">\(\omega &gt; F(x)\)</span>, then since <span class="math inline">\(F\)</span> is right continuous, there is an <span class="math inline">\(\epsilon &gt;0\)</span> so that <span class="math inline">\(F(x + \epsilon) &lt; \omega\)</span> and <span class="math inline">\(X(\omega) \geq x + \epsilon &gt; x\)</span>.</p>
</div>
<p>Even though <span class="math inline">\(F\)</span> may not be 1-1 and onto we will call <span class="math inline">\(X\)</span> the inverse of <span class="math inline">\(F\)</span> and denote it <span class="math inline">\(F^{-1}\)</span>.</p>
<div class="definition" btit="equal in distribution">
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> induce the same distribution <span class="math inline">\(\mu\)</span> on <span class="math inline">\((\mathbb{R}, \mathcal{R})\)</span>, we say <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>equal in distribution</strong>. This holds if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have the same distribution function, <span class="math inline">\(i.e.\)</span>, <span class="math inline">\(P(X\leq x) = P(Y \leq x)\)</span> for all <span class="math inline">\(x\)</span>. When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have the same distribution, we like to write <span class="math display">\[
X \stackrel{d}{=}Y.
\]</span></p>
</div>
<div class="definition" btit="density function">
<p>We say that <span class="math inline">\(X\)</span> has <strong>density function</strong> <span class="math inline">\(f\)</span>. In remembering formulas, it is often useful to think of <span class="math inline">\(f(x)\)</span> as being <span class="math inline">\(P(X = x)\)</span> although <span class="math display">\[
P(X=x)=\lim_{\epsilon\rightarrow 0}\int_{x-\epsilon}^{x+\epsilon}f(y)dy=0.
\]</span></p>
</div>
<p>By popular demand, we have ceased our previous practice of writing <span class="math inline">\(P(X = x)\)</span> for the density function. Instead we will use things like the lovely and informative <span class="math inline">\(f_{X}(x)\)</span>.</p>
<p>A distribution function <span class="math inline">\(\mathbb{R}\)</span> is said to be</p>
<ul>
<li><p><strong>absolutely continuous</strong> if it has a density, and</p></li>
<li><p><strong>singular</strong> if the corresponding measure is singular w.r.t. Lebesgue measure.</p></li>
</ul>
<div class="definition" btit="singular measures">
<p>(Page 405, <span class="citation">Durrett (2019)</span>) Two measures <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> are said to be <strong>mutually singular</strong> if there is a set <span class="math inline">\(A\)</span> with <span class="math inline">\(\mu_{1}(A) = 0\)</span> and <span class="math inline">\(\mu_{2}(A^{c}) = 0\)</span>. In this case, we also say <span class="math inline">\(\mu_{1}\)</span> is <strong>singular with respect to</strong> <span class="math inline">\(\mu_{2}\)</span> an write <span class="math inline">\(\mu_{1} \perp \mu_{2}\)</span>.</p>
</div>
</div>
<div id="random-variables" class="section level2">
<h2><span class="header-section-number">1.4</span> Random variables</h2>
<p>The below definition is an extended version of random variable, which may take the value <span class="math inline">\(+\infty\)</span> or <span class="math inline">\(-\infty\)</span>.</p>
<div class="definition" btit="(an extended version of) random variable">
<p>A function whose domain is a set <span class="math inline">\(D \in \mathcal{F}\)</span> and whose range is <span class="math inline">\(\mathbb{R}^{*} \equiv [-\infty, \infty]\)</span> is said to be a <strong>random variable</strong> if for all <span class="math inline">\(B \in \mathcal{R}^{*}\)</span> we have <span class="math display">\[X^{-1}(B) = \{ \omega : X(\omega) \in B\} \in \mathcal{F}.\]</span> Here <span class="math inline">\(\mathcal{R}^{*}\)</span>= the Borel subsets of <span class="math inline">\(\mathbb{R}^{*}\)</span> with <span class="math inline">\(\mathbb{R}^{*}\)</span> given the usual topology, i.e., the one generated by intervals of the form <span class="math inline">\([-\infty, a), (a,b)\)</span> and <span class="math inline">\((b, \infty]\)</span> where <span class="math inline">\(a,b \in \mathbb{R}\)</span>.</p>
</div>
<p>Note that the <strong>extended real line</strong> <span class="math inline">\((\mathbb{R}^{*}, \mathcal{R}^{*})\)</span> is a measurable space, so all the results above generalize immediately.</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-17">Question</a>
</h4>
</div>
<div id="unnamed-chunk-17" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2007-Summer
</div>
<div class="snu">
2008-Winter
</div>
State the random variable.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-18">Question</a>
</h4>
</div>
<div id="unnamed-chunk-18" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2007-Summer
</div>
Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables on <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> and let <span class="math inline">\(A\in\mathcal{F}\)</span>. Show that <span class="math inline">\(Z(\omega) = X(\omega)\)</span> for <span class="math inline">\(\omega \in A\)</span> and <span class="math inline">\(Z(\omega) = Y(\omega)\)</span> for <span class="math inline">\(\omega \in A^c\)</span>, then <span class="math inline">\(Z\)</span> is a random variable.
</div>
</div>
</div>
</div>
<div class="definition" btit="measurable map">
<p>A function <span class="math inline">\(X: \Omega \rightarrow S\)</span> is said to be a <strong>measurable map</strong> from (<span class="math inline">\(\Omega\)</span>)</p>
</div>
<p>If <span class="math inline">\((S,\mathcal{S}) = (\mathbb{R}^{d}, \mathcal{R}^{d})\)</span> and <span class="math inline">\(d&gt;1\)</span>, then <span class="math inline">\(X\)</span> is called a <strong>random vector</strong>. Of course, if <span class="math inline">\(d=1\)</span>, <span class="math inline">\(X\)</span> is called a <strong>random variable</strong>, or r.v. for short.</p>
<p>The next result is useful for proving that maps are measurable.</p>
<div class="theorem" btit="generator">
<p>If <span class="math inline">\(\{ \omega : X(\omega) \in A \}\in\mathcal{F}\)</span> for all <span class="math inline">\(A \in \mathcal{A}\)</span> and <span class="math inline">\(\mathcal{A}\)</span> <strong>generates</strong> <span class="math inline">\(\mathcal{S}\)</span> (i.e., <span class="math inline">\(\mathcal{S}\)</span> is the smallest <span class="math inline">\(\sigma\)</span>-field that containes <span class="math inline">\(\mathcal{A}\)</span>), then <span class="math inline">\(X\)</span> is measurable.</p>
</div>
<div class="proof">
<p>Writing <span class="math inline">\(\{X \in B\}\)</span> as shorthand for <span class="math inline">\(\{\omega : X(\omega) \in B\}\)</span>, we have <span class="math display">\[\begin{align}
\{X \in \cup_{i}B_i\} &amp;= \cup_{i}\{X\in B_i\}\\
\{X \in B^c\} &amp;= \{X\in B\}^c.
\end{align}\]</span> So the class of sets <span class="math inline">\(\mathcal{B} = \{B : \{X\in B\} \in \mathcal{F}\}\)</span> is a <span class="math inline">\(\sigma\)</span>-field. Since <span class="math inline">\(\mathcal{B} \supset \mathcal{A}\)</span> and <span class="math inline">\(\mathcal{A}\)</span> generates <span class="math inline">\(\mathcal{S}\)</span>.</p>
</div>
<div class="definition" btit="sigma-field generated by a random variable">
<p>It follows from the two equations displayed in the previous proof that if <span class="math inline">\(\mathcal{S}\)</span> is a <span class="math inline">\(\sigma\)</span>-field, then <span class="math inline">\(\{\{ X\in B\} : B\in\mathcal{S} \}\)</span> is a <span class="math inline">\(\sigma\)</span>-field. It is the smallest <span class="math inline">\(\sigma\)</span>-field on <span class="math inline">\(\Omega\)</span> that makes <span class="math inline">\(X\)</span> a measurable map. It is called the <span class="math inline">\(\sigma\)</span>-<strong>field generated by</strong> <span class="math inline">\(X\)</span> and denoted <span class="math inline">\(\sigma(X)\)</span>. For future reference we note that <span class="math display">\[
\sigma(X) = \{\{ X\in B\}: B\in \mathcal{S}\}.
\]</span></p>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-19">Question</a>
</h4>
</div>
<div id="unnamed-chunk-19" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2012-Summer
</div>
Verify that If <span class="math inline">\(X_1, X_2,\ldots\)</span> are random variables then so are <span class="math display">\[
\inf_n X_n \quad{} \sup_{n} X_n \quad{} \text{lim sup}_{n}X_n \quad \text{lim inf}_{n}X_n.
\]</span>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class = "panel" style = "background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-20">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-20" class="panel-collapse collapse">
<div class="panel-body" style = "color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Durrett (2019)</span>, Theorem 1.3.7, page 14)</p>
<ul>
<li><p>Since infimum of a sequence is <span class="math inline">\(&lt;a\)</span> if and only if some term is <span class="math inline">\(&lt;a\)</span> (if all terms are <span class="math inline">\(\geq a\)</span> then the infimum is), we have <span class="math display">\[
\{\inf_{n}X_n &lt;a \} = \cup_n \{ X_n &lt; a\} \in \mathcal{F}.
\]</span></p></li>
<li><p>A similar argument shows <span class="math inline">\(\{sup_n X_n &gt; a\} =\cup_n \{X_n &gt;a \} \in \mathcal{F}\)</span>.</p></li>
<li><p>For the last two, we observe <span class="math display">\[
\begin{align}
\text{lim inf}_{n\rightarrow \infty} X_n &amp;= \sup_{n} \Big( \inf_{m\geq n} X_m \Big)\\
\text{lim sup}_{n\rightarrow \infty} X_n &amp;= \inf_{n} \Big( \sup_{m\geq n} X_m \Big).
\end{align}
\]</span> To complete the proof in the first case, note that <span class="math inline">\(Y_n = \inf_{m\geq n} X_m\)</span> is a random variable for each <span class="math inline">\(n\)</span> so <span class="math inline">\(\sup_n Y_n\)</span> is as well.</p>
</div>
</div>
</div>
</div></li>
</ul>
</div>
<div id="integral" class="section level2">
<h2><span class="header-section-number">1.5</span> Integral</h2>
<p>In this section we will define <span class="math inline">\(\int f d\mu\)</span> for a class of measurable functions. This is a four-step procedure:</p>
<ol style="list-style-type: decimal">
<li><p>Simple functions</p></li>
<li><p>Bounded functions</p></li>
<li><p>Nonnegative functions</p></li>
<li><p>General functions</p></li>
</ol>
<div id="simple-functions" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Simple functions</h3>
<p><span class="math inline">\(\varphi\)</span> is said to be a <strong>simple function</strong> if <span class="math inline">\(\varphi(\omega) = \sum_{i=1}^{n}a_{i}1_{A_i}\)</span> and <span class="math inline">\(A_i\)</span> are disjoint sets with <span class="math inline">\(\mu(A_i) &lt; \infty\)</span>. If <span class="math inline">\(\varphi\)</span> is a simple function, we let <span class="math display">\[
\int \varphi d\mu =\sum_{i=1}^{n}a_i \mu(A_i).
\]</span></p>
<div class="definition" btit="almost everywhere function">
<p><span class="math inline">\(\varphi \geq \psi\)</span> <span class="math inline">\(\mu\)</span>-<strong>almost everywhere</strong> (or <span class="math inline">\(\varphi \geq \psi\)</span> <span class="math inline">\(\mu\)</span>-a.e.) means <span class="math inline">\(\mu(\{\omega : \varphi(\omega) &lt; \psi (\omega)\})=0\)</span>.</p>
</div>
<p>When there is no doubt about what measure we are referring to, we drop the <span class="math inline">\(\mu\)</span>.</p>
</div>
</div>
<div id="properties-of-the-integral" class="section level2">
<h2><span class="header-section-number">1.6</span> Properties of the Integral</h2>
<div class="theorem" btit="Jansen&#39;s inequality">
<p>Suppose <span class="math inline">\(\varphi\)</span> is <strong>convex</strong>, that is <span class="math display">\[
\lambda \varphi (x) + (1-\lambda)\varphi(y) \geq \varphi(\lambda x + (1-\lambda)y),
\]</span> for all <span class="math inline">\(\lambda \in (0,1)\)</span> and <span class="math inline">\(x,y\in\mathbb{R}\)</span>. If <span class="math inline">\(\mu\)</span> is a probability measure, and <span class="math inline">\(f\)</span> and <span class="math inline">\(\varphi(f)\)</span> are integrable, then <span class="math display">\[
\varphi\Bigg( \int f d\mu\Bigg) \leq \int \varphi(f)d\mu.
\]</span></p>
</div>
<div class="theorem" btit="Holder&#39;s inequality">
<p>If <span class="math inline">\(p,q\in (1,\infty)\)</span> with <span class="math inline">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>, then <span class="math display">\[
\int |fg| d\mu \leq \| f \|_{p} \|g\|_{q}.
\]</span></p>
</div>
<div class="remark">
<p>The special case <span class="math inline">\(p=q=2\)</span> is called the <strong>Cauchy-Schwarz inequality</strong>.</p>
</div>
<p>Our next goal is to give conditions that guarantee <span class="math display">\[
\lim_{n\rightarrow \infty} \int f_{n}d\mu =\int \Big( \lim_{n\rightarrow \infty} f_n\Big) d\mu.
\]</span> First, we need a definition. We say that <span class="math inline">\(f_n \rightarrow f\)</span> <strong>in measure</strong>, i.e., for any <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math inline">\(\mu (\{x : |f_{n}(x) - f(x) | &gt;\epsilon \})\rightarrow 0\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. On a space of finite measure, this is a weaker assumption than <span class="math inline">\(f_n \rightarrow f\)</span> a.e., but the next result is easier to prove in the great generality.</p>
<div class="theorem" btit="bounded convergence theorem">
<p>Let <span class="math inline">\(E\)</span> be a set with <span class="math inline">\(\mu (E) &lt; \infty\)</span>. Suppose <span class="math inline">\(f_n\)</span> vanishes on <span class="math inline">\(E^c\)</span>, <span class="math inline">\(|f_n (x)|\leq M\)</span>, and <span class="math inline">\(f_n \rightarrow f\)</span> in measure. Then, <span class="math display">\[
\int f d\mu = \lim_{n\rightarrow \infty}\int f_n d\mu.
\]</span></p>
</div>
<p>The statement of Fatou’s lemma comes from Theorem 2.5.2 of <span class="citation">Gut (2014)</span> .</p>
<div class="lemma" btit="Fatou&#39;s lemma">
<p>(i). If <span class="math inline">\(\{X_n , n\geq 1\}\)</span> are non-negative random variables, then <span class="math display">\[
E \text{lim inf}_{n\rightarrow \infty} X_n \leq \text{lim inf}_{n\rightarrow \infty} E X_n.
\]</span></p>
<p>(ii). If, in addition, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> are integrable random variables, such that <span class="math inline">\(Y\leq X_n \leq Z\)</span> a.s. for all <span class="math inline">\(n\)</span>, then <span class="math display">\[
E \text{lim inf}_{n\rightarrow} X_n \leq \text{lim inf}_{n\rightarrow \infty}EX_n \leq \text{lim sup}_{n\rightarrow \infty} EX_n \leq E\text{lim sup}_{n\rightarrow \infty} X_n.
\]</span></p>
</div>
<p>Note that when <span class="math inline">\(\{A_n , n\geq 1\}\)</span> is non-decreasing, <span class="math inline">\(A_n \nearrow\)</span>, if <span class="math inline">\(A_1 \subset A_2 \subset \cdots\)</span>; Similarly, <span class="math inline">\(\{A_n , n\geq 1\}\)</span> is non-increasing, <span class="math inline">\(A_n \searrow\)</span>, if <span class="math inline">\(A_1 \supset A_2 \supset \cdots\)</span>. (page 4, <span class="citation">(Gut 2014)</span>)</p>
<div class="proof">
<p>(i). Set <span class="math inline">\(Y_n = \inf_{k\geq n} X_{k}, n\geq 1\)</span>. Since <span class="math display">\[
Y_n =\inf_{k\geq n}X_k \nearrow \text{lim inf}_{n\rightarrow\infty}X_n \quad{\text{as } n\rightarrow 0,}
\]</span> the monotone convergence theorem yields <span class="math display">\[
EY_n \nearrow E\text{lim inf}_{n\rightarrow \infty} X_n.
\]</span> Moreover, since <span class="math inline">\(Y_n \leq X_n\)</span>, the donimaion theorem of random variables (when <span class="math inline">\(X,Y\)</span> be non-negative random variables, if <span class="math inline">\(Y\leq X\)</span> a.s., then <span class="math inline">\(E(Y)\leq E(X)\)</span>, page 52, Theorem 4.4. of <span class="citation">Gut (2014)</span>) tells us that <span class="math display">\[
EY_n \leq EX_n \quad{\forall n}.
\]</span> Combining the two proves (i).</p>
<p>To prove (ii) we begin by noticing that <span class="math display">\[
\text{lim inf}_{n\rightarrow\infty}(X_n - Y) = \text{lim inf}_{n\rightarrow\infty}X_n - Y 
\]</span> and <span class="math display">\[
\text{lim inf}_{n\rightarrow\infty}(Z-X_n) = Z-\text{lim sup}_{n\rightarrow\infty}X_n , 
\]</span> after which (ii) follows from (i) and additivity, since <span class="math inline">\(\{X_n - Y, n\geq 1\}\)</span> and <span class="math inline">\(\{Z - X_n, n\geq 1\}\)</span> are non-negative random variables.</p>
</div>
<p>A typical use of Fatou’s lemma is in cases where one knows that a pointwise limit exists, and it is enough to assert that the expected value of the limit is finite.</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-21">Question</a>
</h4>
</div>
<div id="unnamed-chunk-21" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="kaist">
2012-Summer
</div>
<div class="kaist">
2020-Winter
</div>
Verify that <span class="math display">\[
P\{\text{lim inf}_{n\rightarrow} A_n\} \leq \text{lim inf}_{n\rightarrow \infty}P\{A_n\} \leq \text{lim sup}_{n\rightarrow \infty} P\{A_n\} \leq P\{\text{lim sup}_{n\rightarrow \infty} A_n\} = P(A_n \text{ infinitely often}).
\]</span>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-22">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-22" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Gut (2014)</span>, Theorem 1.3.2, page 12, (i)) (<span class="citation">Durrett (2019)</span>, Exercise 2.3.1) By definition, for any <span class="math inline">\(n\)</span>, we obtain from the below proposition, <span class="math display">\[
\text{lim inf}_{n\rightarrow \infty}A_n \nwarrow \bigcap_{m=n}^{\infty}A_m \subset A_n \subset \bigcup_{m=n}^{\infty}A_m \searrow \text{lim sup}_{n\rightarrow \infty}A_n,
\]</span> where the limits are taken as <span class="math inline">\(n\rightarrow \infty\)</span>. Joining this with the below theorem, yields <span class="math display">\[
P\{\text{lim inf}_{n\rightarrow} A_n\} \leq \text{lim inf}_{n\rightarrow \infty}P\{A_n\} \leq \text{lim sup}_{n\rightarrow \infty} P\{A_n\} \leq P\{\text{lim sup}_{n\rightarrow \infty} A_n\}.
\]</span></p>
<div class="proposition" btit="existence of limit of monotone set">
<p>(<span class="citation">Gut (2014)</span>, Theorem 1.2.1, page 5) Let <span class="math inline">\(\{A_n , n\geq 1\}\)</span> be a sequence of subsets of <span class="math inline">\(\Omega\)</span>.</p>
<p>(i). If <span class="math inline">\(A_1 \subset A_2 \subset A_3 \cdots\)</span>, then <span class="math display">\[
\lim_{n\rightarrow \infty}A_n =\bigcup_{n=1}^{\infty}A_n.
\]</span></p>
<p>(ii). If <span class="math inline">\(A_1 \supset A_2 \supset A_3 \cdots\)</span>, then <span class="math display">\[
\lim_{n\rightarrow \infty}A_n =\bigcap_{n=1}^{\infty}A_n.
\]</span></p>
</div>
<div class="theorem" btit="convergence of probabilities of converging sets">
<p>(<span class="citation">Gut (2014)</span>, Remark 1.3.1) Suppose that <span class="math inline">\(A\)</span> and <span class="math inline">\(\{A_n n\geq 1\}\)</span> are subsets of <span class="math inline">\(\Omega\)</span>, such that <span class="math inline">\(A_n \nearrow A\)</span> (<span class="math inline">\(A_n \searrow A\)</span>) as <span class="math inline">\(n\rightarrow \infty\)</span>. Then <span class="math display">\[
P(A_n) \nearrow P(A) \quad{(P(A_n)\searrow P(A)) \quad{\text{ as } n\rightarrow \infty.}}
\]</span></p>
</div>
</div>
</div>
</div>
</div>
<div class="remark">
<p>(<span class="citation">Gut (2014)</span>, Remark 2.5.3) If the random variables are indicators, the result transforms into an inequality for probabilities and we can proof the above problem. Technically, if <span class="math inline">\(X_n = I\{A_n\}, n\geq 1\)</span>, then (i) reduces to <span class="math inline">\(P(\text{lim inf}_{n\rightarrow \infty}A_n)\leq \text{lim inf}_{n\rightarrow\infty}P(A_n)\)</span>, and so on.</p>
</div>
<div class="theorem" btit="monotone convergence theorem">
<p>If <span class="math inline">\(f_n \geq 0\)</span> and <span class="math inline">\(f_n \uparrow f\)</span>, then <span class="math display">\[
f_n d\mu \uparrow \int f d\mu.
\]</span></p>
</div>
<div class="theorem" btit="dominated convergence theorem">
<p>If <span class="math inline">\(f_n \rightarrow f\)</span> a.e., <span class="math inline">\(|f_n| \leq g\)</span> for all <span class="math inline">\(n\)</span>, and <span class="math inline">\(g\)</span> is integrable, then <span class="math inline">\(\int f_n d\mu \rightarrow \int f d\mu\)</span>.</p>
</div>
</div>
<div id="expected-value" class="section level2">
<h2><span class="header-section-number">1.7</span> Expected Value</h2>
<p>We now specialize to integration with respect to a proability measure <span class="math inline">\(P\)</span>. If <span class="math inline">\(X\geq 0\)</span> is a random variable on <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>, then we define its <strong>expected value</strong> to be <span class="math display">\[EX = \int X dP,\]</span> which always makes sense, but may be <span class="math inline">\(\infty\)</span>. To reduce the general case to the nonnegative case, let <span class="math inline">\(x^{+} = \max \{ x, 0\}\)</span> be the <strong>positive part</strong> and let <span class="math inline">\(x^{-} =\max\{ -x , 0\}\)</span> be the <strong>negative part</strong> of <span class="math inline">\(x\)</span>. We declare that <span class="math inline">\(EX\)</span> <strong>exists</strong> and set <span class="math inline">\(EX = EX^{+} - EX^{-}\)</span> whenever the subtraction makes sense, i.e., <span class="math inline">\(EX^{+}&lt;\infty\)</span> or <span class="math inline">\(EX^{-}&lt;\infty\)</span>.</p>
<p><span class="math inline">\(EX\)</span> is often called the <strong>mean</strong> of <span class="math inline">\(X\)</span> and denoted by <span class="math inline">\(\mu\)</span>. <span class="math inline">\(EX\)</span> is defined by integrating <span class="math inline">\(X\)</span>, so it has all the properties that integrals do.</p>
<div id="integration-to-the-limit" class="section level3">
<h3><span class="header-section-number">1.7.1</span> Integration to the limit</h3>
<div class="theorem" btit="Fatou&#39;s lemma">
<p>If <span class="math inline">\(X_n \geq 0\)</span>, then <span class="math display">\[
\text{lim inf}_{n\rightarrow \infty}EX_n \geq E(\text{lim inf}_{n\rightarrow \infty}X_n).
\]</span></p>
</div>
<div class="theorem" btit="monotone convergence theorem">
<p>If <span class="math inline">\(0 \leq X_n \uparrow X\)</span>, then <span class="math inline">\(EX_n \uparrow EX\)</span>.</p>
</div>
<div class="theorem" btit="dominated convergence theorem">
<p>If <span class="math inline">\(X_n \rightarrow X\)</span> a.s., <span class="math inline">\(|X_n|\leq Y\)</span> for all <span class="math inline">\(n\)</span>, and <span class="math inline">\(EY&lt;\infty\)</span>, then <span class="math inline">\(EX_n \rightarrow EX\)</span>.</p>
</div>
<p>The special case of the dominated convergence theorem where <span class="math inline">\(Y\)</span> is constant is called the <strong>bounded convergence theorem</strong>.</p>
<p>In the developments that follow, we will need another result on integration to the limit. Perhaps the most important special case of this result occurs when <span class="math inline">\(g(x) = |x|^{p}\)</span> with <span class="math inline">\(p&gt;1\)</span> and <span class="math inline">\(h(x) = x\)</span>.</p>
<div class="theorem" btit="continuous function and expectstion">
<p>Suppose <span class="math inline">\(X_n \rightarrow X\)</span> a.s. Let <span class="math inline">\(g, h\)</span> be continuous functions with</p>
<p>(i). <span class="math inline">\(g\geq 0\)</span> and <span class="math inline">\(g(x) \rightarrow \infty\)</span> as <span class="math inline">\(|x| \rightarrow \infty\)</span>,</p>
<p>(ii). <span class="math inline">\(\frac{|h(x)|}{g(x)}\rightarrow 0\)</span> as <span class="math inline">\(|x|\rightarrow \infty\)</span>, and</p>
<p>(iii). <span class="math inline">\(Eg(X_n) \leq K &lt;\infty\)</span> for all <span class="math inline">\(n\)</span>.</p>
</div>
</div>
<div id="computing-expected-values" class="section level3">
<h3><span class="header-section-number">1.7.2</span> Computing Expected Values</h3>
<p>Integrating over <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> is nice in theory, but to do computations we have to shift to a space on which we can do calculus. In most cases, we will apply the next result with <span class="math inline">\(S =\mathbb{R}^d\)</span>.</p>
<div class="theorem" btit="change of variables formula">
<p>Let <span class="math inline">\(X\)</span> be a random element of <span class="math inline">\((S, \mathcal{S})\)</span> with distribution <span class="math inline">\(\mu\)</span>, i.e., <span class="math inline">\(\mu(A) = P(X\in A)\)</span>. If <span class="math inline">\(f\)</span> is a measurable function from <span class="math inline">\((S, \mathcal{S})\)</span> to <span class="math inline">\((\mathbb{R}, \mathcal{R})\)</span> so that <span class="math inline">\(f \geq 0\)</span> or <span class="math inline">\(E|f(X)|&lt;\infty\)</span>, then <span class="math display">\[
Ef(X) = \int_{S}f(y)\mu(dy).
\]</span></p>
</div>
<div class="remark">
<p>To explain the name, write, <span class="math inline">\(h\)</span> for <span class="math inline">\(X\)</span> and <span class="math inline">\(P \circ h^{-1}\)</span> for <span class="math inline">\(\mu\)</span> to get <span class="math display">\[
\int_{\Omega} f(h(\omega))dP = \int_{S} f(y) d(P\circ h^{-1}).
\]</span></p>
</div>
</div>
</div>
<div id="product-measures-fubinis-theorem" class="section level2">
<h2><span class="header-section-number">1.8</span> Product Measures, Fubini’s Theorem</h2>
<p>Let <span class="math inline">\((X, \mathcal{A}, \mu_1)\)</span> and <span class="math inline">\((Y, \mathcal{B}, \mu_{2})\)</span> be two <span class="math inline">\(\sigma\)</span>-finite measure spaces. Let <span class="math display">\[\begin{align}
\Omega &amp;= X \times Y = \{(x,y) : x \in X, y \in Y\},\\
\mathcal{S} &amp;= \{ A \times B: A \in \mathcal{A}, B \in \mathcal{B}\}.
\end{align}\]</span></p>
<p>Sets in <span class="math inline">\(\mathcal{S}\)</span> are called <strong>rectangles</strong>. It is easy to see that <span class="math inline">\(\mathcal{S}\)</span> is a semi-algebra: <span class="math display">\[\begin{align}
&amp;(A\times B) \cap (C \times D) = (A \cap C) \times (B \cap D)\\
&amp;(A\times B)^{c} = (A^c \times B) \cup (A \times B^c) \cup (A^c \times B^c).
\end{align}\]</span></p>
<p>Let <span class="math inline">\(\mathcal{F} = \mathcal{A} \times \mathcal{B}\)</span> be the <span class="math inline">\(\sigma\)</span>-algebra generated by <span class="math inline">\(\mathcal{S}\)</span>.</p>
<div class="theorem" btit="existence of a unique product measure">
<p>There is a unique measure <span class="math inline">\(\mu\)</span> on <span class="math inline">\(\mathcal{F}\)</span> with <span class="math display">\[
\mu(A \times B) = \mu_1 (A) \mu_2 (B).
\]</span></p>
</div>
<p><span class="math inline">\(\mu\)</span> is often denoted by <span class="math inline">\(\mu_1 \times \mu_2\)</span>.</p>
<p>When <span class="math inline">\((\Omega, \mathcal{F}, \mu)\)</span> is the product of two measure spaces, <span class="math inline">\((X, \mathcal{A}, \mu)\)</span> and <span class="math inline">\((Y, \mathcal{B}, \nu)\)</span>, our next goal is to prove:</p>
<div class="theorem" btit="Fubini&#39;s theorem">
<p>If <span class="math inline">\(f\geq 0\)</span> or <span class="math inline">\(\int |f| d\mu &lt; \infty\)</span>, then <span class="math display">\[
\int_{X}\int_Y f(x,y)\mu_{2}(dy)\mu_{1}(dx) = \int_{X\times Y}f d\mu = \int_{Y}\int_{X}f(x,y)\mu_{1}(dx)\mu_{2}(dy).
\]</span></p>
</div>
</div>
<div id="inequalities" class="section level2">
<h2><span class="header-section-number">1.9</span> Inequalities</h2>
<div id="tail-probabilities-estimated-via-moments" class="section level3">
<h3><span class="header-section-number">1.9.1</span> Tail Probabilities Estimated Via Moments</h3>
<div class="theorem" btit="Sandwiching E|X|">
<p>(Page 153, Inequality of <span class="citation">Shorack (2017)</span>)</p>
<p>For any r.v. <span class="math inline">\(X\)</span>, we have</p>
<p>(1). $<em>{n=1}^{}P(|X|n) E|X| = </em>{0}^{}P(|X|&gt;x)dx _{n=0}^{}P(|X|n).</p>
<p>If <span class="math inline">\(X\)</span> is a r.v. with values <span class="math inline">\(0, 1,2,\ldots\)</span>, then</p>
<p>(2). $E(X) = _{n=1}^{}P(Xn).</p>
</div>
<div class="proof">
<p>If <span class="math inline">\(X\geq 0\)</span>, then <span class="math inline">\(EX = \int_{0}^{\infty}P(X&gt;x)dx = \int_{0}^{\infty}[1-F(x)]dx\)</span>. If <span class="math inline">\(X\geq 0\)</span> is integer valued, then <span class="math display">\[
\begin{align}
E(X)&amp;=\sum_{k=0}^{\infty}kP(X=k)=\sum_{k=1}^{\infty}\sum_{n=1}^{k}P(X=k)\\
&amp;= \sum_{n=1}^{\infty}\sum_{k=n}^{\infty}P(X=k)\\
&amp;= \sum_{n=1}^{\infty}P(X\geq n).
\end{align}
\]</span> For the greates integer function <span class="math inline">\([\cdot]\)</span>, an arbitrary r.v. satisfies <span class="math display">\[
[|X|] \leq |X| \leq [|X|]+1.
\]</span> Moreover, <span class="math display">\[
E[|X|] = \sum_{n=1}^{\infty}P([|X|]\geq n) = \sum_{n=1}^{\infty}P(|X|\geq n),
\]</span> while <span class="math display">\[
E\{[|X|] + 1 \} = \sum_{n=1}^{\infty}P(|X|\geq n) + 1 = \sum_{n=1}^{\infty}P(|X|\geq n) + P(|X| \geq 0).
\]</span></p>
<div class="figure">
<img src="images/sandwitch.png" alt="" />
<p class="caption">The moment E|X| is sandwiched.</p>
</div>
</div>
<p>To state our next result, we need some notation. If we only integrate over <span class="math inline">\(A\subset \Omega\)</span>, we write <span class="math display">\[
E(X;A)=\int_{A}XdP.
\]</span></p>
<div class="definition" btit="Chebyshev&#39;s inequality">
<p>Suppose <span class="math inline">\(\varphi: \mathbb{R} \rightarrow \mathbb{R}\)</span> has <span class="math inline">\(\varphi \geq 0\)</span>, let <span class="math inline">\(A \in \mathcal{R}\)</span> and let <span class="math inline">\(i_{A} = \inf\{ \varphi(y): y\in A\}\)</span>. Then <span class="math display">\[
i_{A}P(X\in A) \leq E(\varphi(X); X\in A)\leq E\varphi(X).
\]</span></p>
</div>
<div class="remark">
<p><strong>Markov’s inequality</strong> is the special case of Chebyshev’s ineaulity, where <span class="math inline">\(\varphi(x) =x^2\)</span> and <span class="math inline">\(A =\{ x : |x| \geq a\}\)</span>: <span class="math display">\[
a^{2}P(|X| \geq a) \leq EX^2.
\]</span></p>
</div>
</div>
<div id="moment-inequalities" class="section level3">
<h3><span class="header-section-number">1.9.2</span> Moment Inequalities</h3>
<p>Next, some inequalities that relate moments of sums to sums of moments. Note that we do not assume independence between the summands.</p>
<div class="theorem" btit="Theorem 3.2.1 of Gut 2014">
<p>Let <span class="math inline">\(r &gt; 0\)</span>. Suppose that <span class="math inline">\(E|X|^{r} &lt; \infty\)</span> and <span class="math inline">\(E|Y|^{r} &lt;\infty\)</span>. Then <span class="math display">\[
E|X+Y|^{r} \leq 2^{r}(E|X|^r + E|Y|^r).
\]</span></p>
</div>
<div class="proof">
<p>Set <span class="math inline">\(x = X(\omega)\)</span> and <span class="math inline">\(y = Y(\omega)\)</span>. The triangle inequality and Lemma A.5.1 of <span class="citation">Gut (2014)</span> together yield <span class="math display">\[
E|X+Y|^{r} \leq E(|X|+|Y|)^{r}\leq 2^r (E|X|^r + E|Y|^r).
\]</span></p>
</div>
<p>Although the inequality is enough for many purposes, a sharper one can be obtained as follows.</p>
<div class="theorem" btit="$c_r$-inequality">
<p>(Theorem 3.2.2 of <span class="citation">Gut (2014)</span>) Let <span class="math inline">\(r &gt; 0\)</span>. Suppose that <span class="math inline">\(E|X|^{r} &lt; \infty\)</span> and <span class="math inline">\(E|Y|^{r} &lt;\infty\)</span>. Then <span class="math display">\[
E|X+Y|^{r} \leq c^{r}(E|X|^r + E|Y|^r).
\]</span></p>
</div>
<div class="proof">
<p>Set <span class="math inline">\(x= X(\omega)\)</span> and <span class="math inline">\(y=Y(\omega)\)</span> for <span class="math inline">\(\omega \in \Omega\)</span>. By the triangle inequality and the second inequality of Lemma A.5.1 of <span class="citation">Gut (2014)</span>, <span class="math display">\[
E|X+Y|^{r} \leq E(|X| +|Y|)^{r} \leq E|X|^{r} + E|Y|^{r},
\]</span> which established the inequality for the case <span class="math inline">\(0 &lt; r \leq 1\)</span>.</p>
<p>For <span class="math inline">\(r\geq 1\)</span> the desired inequality follows the same procedure with the second inequality of Lemma A.5.1 replaced by the third one.</p>
</div>
<p>The last two results tell us that the if the summands are integrable, then so is the sum. The integrability assumption is of course superfluous, but without it the right-hand side would be infinite, and the result would be void.</p>
<p>There is no general converse to that statement. If both variables are non-negative the converse is trivial, since, then, <span class="math inline">\(X\leq X+Y\)</span>. However, let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be Cauchy-distributed, say, and such that <span class="math inline">\(Y=-X\)</span>. Then <span class="math inline">\(X+Y\)</span> equals <span class="math inline">\(0\)</span> and thus has moments all orders, but <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> do not.</p>
<p>However, for independent summands there exists a converse.</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-23">Question</a>
</h4>
</div>
<div id="unnamed-chunk-23" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2013-Winter
</div>
<div class="theorem" btit="Theorem 3.2.3 of Gut 2014">
<p>Let <span class="math inline">\(r &gt;0\)</span>. If <span class="math inline">\(E|X+Y|^{r} &lt; \infty\)</span> and <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(E|X|^{r} &lt;\infty\)</span> and <span class="math inline">\(E|Y|^{r}&lt;\infty\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-24">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-24" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(Page 127, Theorem 3.2.3 of <span class="citation">Gut (2014)</span>)</p>
<p>By assumption, <span class="math display">\[
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} |x+y|^{r} dF(x) dF(y) &lt;\infty.
\]</span> The finiteness of the double integral implies that the inner integral must be finite for at least one <span class="math inline">\(y\)</span> (in fact, for almost all <span class="math inline">\(y\)</span>). Therefore, pick <span class="math inline">\(y\)</span>, such that <span class="math display">\[
\int_{-\infty}^{\infty}|x+y|^{r}dF(x) &lt;\infty,
\]</span> which means that <span class="math display">\[
E|X+y|^{r}&lt;\infty.
\]</span> An application of the <span class="math inline">\(c_r\)</span>-inequality asserts that <span class="math display">\[
E|X|^{r} \leq c_r (E|X+y|^{r} + |y|^r) &lt; \infty.
\]</span> The integrability of <span class="math inline">\(Y\)</span> follows similiarly, or, alternatively, via another application of the <span class="math inline">\(c_r\)</span>-inequality.</p>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-25">Question</a>
</h4>
</div>
<div id="unnamed-chunk-25" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2016-Winter
</div>
<div class="snu">
2017-Summer
</div>
Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent r.v.’s. Show that <span class="math inline">\(E|X+Y|^{p} &lt;\infty\)</span> if and only if <span class="math inline">\(E|X|^{p}+E|Y|^{p} &lt;\infty\)</span>, where <span class="math inline">\(p\)</span> is a positive number.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-26">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-26" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>From Theorem 3.2.1 of <span class="citation">Gut (2014)</span>, when <span class="math inline">\(E|X|^{p}+E|Y|^{p} &lt;\infty\)</span>, then <span class="math inline">\(E|X+Y|^{p} &lt;\infty\)</span>. From Theorem 3.2.3 of <span class="citation">Gut (2014)</span>, when <span class="math inline">\(E|X+Y|^{p} &lt;\infty\)</span>, then <span class="math inline">\(E|X|^{p}+E|Y|^{p} &lt;\infty\)</span>. Note that Theorem 3.2.3 of <span class="citation">Gut (2014)</span> holds when <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>
</div>
</div>
</div>
</div>
<div class="theorem" btit="Holder&#39;s inequality">
<p>If <span class="math inline">\(p,q\in [1,\infty]\)</span> with <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span>, then <span class="math display">\[
E|XY| \leq \| X\|_{p}\|Y \|_{q}.
\]</span> Here <span class="math inline">\(\|X\|_{r} = (E|X|^{r})^{1/r}\)</span> for <span class="math inline">\(r \in [1,\infty)\)</span>; <span class="math inline">\(\|X\|_{\infty} = \inf\{ M : P(|X|&gt; M)=0\}\)</span>.</p>
</div>
</div>
<div id="convexity-in-mathematics" class="section level3">
<h3><span class="header-section-number">1.9.3</span> Convexity in Mathematics</h3>
<p>For <span class="math inline">\(x, y\in \mathbb{R}\)</span> the standard triangular inequality states that <span class="math inline">\(|x+y| \leq |x| + |y|\)</span>. Following are some analogs for powers.</p>
<div class="lemma" btit="Lemma A.5.1 of Gut 2014">
<p>Let <span class="math inline">\(r &gt; 0\)</span>, and suppose that <span class="math inline">\(x,y &gt; 0\)</span>. Then <span class="math display">\[
(x+y)^r \leq
\begin{cases}
2^r(x^r+y^r), &amp; \text{for }r &gt;0,\\
x^r+y^r, &amp; \text{for } 0 &lt; r \leq 1,\\
2^{r-1}(x^r + y^r), &amp; \text{for } r \geq 1.
\end{cases}
\]</span></p>
</div>
<div class="proof">
<p>For <span class="math inline">\(r&gt;0\)</span>, <span class="math inline">\((x+y)\)</span>$r (2{x,y})^{r} = 2^{r} ({x,y})^r 2^{r} (x^r + y^r) Next, suppose that <span class="math inline">\(0 &lt; r \leq 1\)</span>. Then, since <span class="math inline">\(x^{\frac{1}{r}}\leq x\)</span> for any <span class="math inline">\(0&lt;x&lt;1\)</span>, it follows that <span class="math display">\[
\Big(\frac{x^r}{x^r+y^r}\Big)^{\frac{1}{r}} +\Big(\frac{y^r}{x^r+y^r}\Big)^{\frac{1}{r}}\leq \frac{x^r}{x^r+y^r} + \frac{y^r}{x^r+y^r}\Big)^{\frac{1}{r}} =1,
\]</span> and, hence, that <span class="math display">\[
x+y \leq (x^r + y^r)^{\frac{1}{r}},
\]</span> which is the same as the second inequality.</p>
<p>For <span class="math inline">\(r \geq 1\)</span> we exploit the fact that the function <span class="math inline">\(|x|^r\)</span> is convex, so that, in particular, <span class="math display">\[
\Big( \frac{x+y}{2}\Big)^{r} \leq \frac{1}{2}x^r + \frac{1}{2}y^r,
\]</span> which is easily reshuffled into the third inequality.</p>
</div>
</div>
<div id="convexity" class="section level3">
<h3><span class="header-section-number">1.9.4</span> Convexity</h3>
<div class="theorem" btit="Jensen&#39;s inequality">
<p>Suppose <span class="math inline">\(\varphi\)</span> is <strong>convex</strong>, that is, <span class="math display">\[
\lambda \varphi(x) + (1-\lambda)\varphi(y) \geq \varphi(\lambda x + (1-\lambda)y)
\]</span> for all <span class="math inline">\(\lambda \in (0,1)\)</span> and <span class="math inline">\(x,y\in\mathbb{R}\)</span>. Then <span class="math display">\[
E(\varphi(X))\geq \varphi(EX)
\]</span> provided both expectations exist, i.e., <span class="math inline">\(E|X|\)</span> and <span class="math inline">\(E|\varphi(X)|&lt;\infty\)</span>.</p>
</div>
</div>
</div>
</div>
<div id="laws-of-large-numbers" class="section level1">
<h1><span class="header-section-number">2</span> Laws of Large Numbers</h1>
<div id="independence" class="section level2">
<h2><span class="header-section-number">2.1</span> Independence</h2>
<div class="definition" btit="independence">
<ul>
<li><p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong>independent</strong> if <span class="math inline">\(P(A\cap B) = P(A)P(B)\)</span>.</p></li>
<li><p>Two random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>independent</strong> if for all <span class="math inline">\(C, D \in \mathcal{R}\)</span>, <span class="math display">\[
P(X\in C, Y \in D) = P(X\in C) P(Y \in D),
\]</span> i.e., the events <span class="math inline">\(A = \{X\in C\}\)</span> and <span class="math inline">\(B = \{Y \in D\}\)</span> are independent.</p></li>
<li><p>Two <span class="math inline">\(\sigma\)</span>-fields <span class="math inline">\(\mathcal{F}\)</span> and <span class="math inline">\(\mathcal{G}\)</span> are <strong>independent</strong> if for all <span class="math inline">\(A \in \mathcal{F}\)</span> and <span class="math inline">\(B \in \mathcal{G}\)</span> the events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent.</p></li>
<li><p>An infinite collection of objects (<span class="math inline">\(\sigma\)</span>-fields, random variables, or sets) is said to be independent if every finite subcollection is.</p></li>
<li><p><span class="math inline">\(\sigma\)</span>-fields <span class="math inline">\(\mathcal{F}_{1},\mathcal{F}_{2},\ldots, \mathcal{F}_n\)</span> are <strong>independent</strong> if whenever <span class="math inline">\(A_i \in\mathcal{F}_{i}\)</span> for <span class="math inline">\(i=1,\ldots, n\)</span>, we have <span class="math display">\[
P(\cap_{i=1}^{n}A_i) = \prod_{i=1}^{n}P(A_i).
\]</span></p></li>
<li><p>Random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> are <strong>independent</strong> if whenever <span class="math inline">\(B_i \in\mathcal{R}\)</span> for <span class="math inline">\(i=1,\ldots, n\)</span>, we have <span class="math display">\[
P(\cap_{i=1}^{n}\{X_i \in B_i\})=\prod_{i=1}^{n}P(X_i \in B_i).
\]</span></p></li>
<li><p>Sets <span class="math inline">\(A_1, \ldots, A_n\)</span> are <strong>independent</strong> if whenever <span class="math inline">\(I \subset \{ 1, \ldots, n\}\)</span> we have <span class="math display">\[
P(\cap_{i\in I}A_i) = \prod_{i\in I}P(A_i).
\]</span></p></li>
</ul>
</div>
<div class="theorem" btit="independence of random variables and their corresponding sigma-fields">
<p>(i). If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent then <span class="math inline">\(\sigma(X)\)</span> and <span class="math inline">\(\sigma(Y)\)</span> are.</p>
<p>(ii). Conversely, if <span class="math inline">\(\mathcal{F}\)</span> and <span class="math inline">\(\mathcal{G}\)</span> are independent, <span class="math inline">\(X\in \mathcal{F}\)</span>, and <span class="math inline">\(Y \in \mathcal{G}\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-27">Question</a>
</h4>
</div>
<div id="unnamed-chunk-27" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2014-Summer
</div>
<div class="snu">
2014-Winter
</div>
<p>Prove or disprove by counter example the following statement:</p>
If <span class="math inline">\(X^2\)</span> and <span class="math inline">\(Y^2\)</span> are independent, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-28">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-28" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(Section 7.6 of <span class="citation">Stoyanov (2014)</span>)</p>
<p>It is well known that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent r.v.s, then for any continuous functions <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span>, the r.v.s <span class="math inline">\(g(X)\)</span> and <span class="math inline">\(h(Y)\)</span> are also independent. The converse statement is true if the functions <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> are one-one mappings of <span class="math inline">\(\mathbb{R}^{1}\)</span> to <span class="math inline">\(\mathbb{R}^{1}\)</span>. However, we can choose functions <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> without this condition such that <span class="math inline">\(g(X)\)</span> and <span class="math inline">\(h(Y)\)</span> are independent r.v.s but <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> themselves are not.</p>
<p>(i). Consider the two-dimensional random vector <span class="math inline">\((X,Y)\)</span> with <span class="math display">\[
p_{i,j}:= P[X=i, Y=j], \quad{i,j=-1,0,1}
\]</span> where <span class="math display">\[
p_{1,1}=p_{-1,1}=\frac{1}{32}, p_{-1,-1} = p_{1,-1} = p_{1,0}=p_{0,1} = \frac{3}{32}\\
p_{-1,0}=p_{0,-1}=\frac{5}{32}, p_{0,0}=\frac{8}{32}.
\]</span> It is easy to check that <span class="math inline">\(X^2\)</span> and <span class="math inline">\(Y^2\)</span> are independent r.v.s. but <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not.</p>
<p>(ii). Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be two independent absolutely continuous r.v.s. Take another r.v. <span class="math inline">\(Y\)</span> which is independent on <span class="math inline">\(X_1, X_2\)</span> and assumes the values <span class="math inline">\(+1\)</span> and <span class="math inline">\(-1\)</span> with probability <span class="math inline">\(\frac{1}{2}\)</span> each. Define two new r.v.s, say <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span>, by <span class="math display">\[
Z_{1} =YX_{1}, Z_2 = YX_{2}.
\]</span> The absolute continuity of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> implies that <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> are absolutely continuous. Obviously, <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> are functionally connected and thus they cannot be independent. However, <span class="math inline">\(Z_{1}^2 = X_{1}^{2}, Z_{2}^{2} = X_{2}^{2}\)</span> and, since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent, <span class="math inline">\(Z_1^2\)</span> and <span class="math inline">\(Z_2^2\)</span> are independent.</p>
<p>(iii). Let the random vector <span class="math inline">\((X,Y)\)</span> have the following density <span class="math display">\[
f(x,y) =
\begin{cases}
\frac{1}{4}(1+xy), &amp; \text{if }|x|&lt; 1 \text{ and } |y|&lt;1\\
0, &amp; \text{o.w.}
\end{cases}
\]</span> We easily find the marginal densities <span class="math inline">\(f_{1}(x)\)</span> of <span class="math inline">\(X\)</span> and <span class="math inline">\(f_{2}(y)\)</span> of <span class="math inline">\(Y\)</span>: <span class="math display">\[
f_{1}(x) = 
\begin{cases}
\frac{1}{2}, &amp; \text{if } |x| &lt; 1\\
0, &amp; \text{o.w.},
\end{cases}
\quad{}
f_{2}(y) =
\begin{cases}
\frac{1}{2}, &amp; \text{if } |y| &lt; 1\\
0, &amp; \text{o.w.}.
\end{cases}
\]</span> Obviously, <span class="math inline">\(f(x,y) \neq f_{1}(x)f_{2}(y)\)</span> for all <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, hence <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are dependent.</p>
<p>Each of the variables <span class="math inline">\(X^2\)</span> and <span class="math inline">\(Y^2\)</span> takes values in <span class="math inline">\((0,1)\)</span> and for <span class="math inline">\(x \in (0,1)\)</span> and <span class="math inline">\(y\in (0,1)\)</span>, we find <span class="math display">\[
\begin{align}
P[X^2 &lt; x, Y^2 &lt; y] &amp;= P[-\sqrt{x} &lt; X &lt; \sqrt{x}, -\sqrt{y} &lt;Y &lt; \sqrt{y}]\\
&amp;= \frac{1}{4}\int_{-\sqrt{x}}^{\sqrt{x}}\int_{-\sqrt{y}}^{\sqrt{y}}(1+uv)dudv\\
&amp;=\sqrt{x} \sqrt{y} = P[X^2 &lt; x]P[Y^2 &lt;y], \quad{x,y\in (0,1).}
\end{align}
\]</span> Thus <span class="math inline">\(X^2\)</span> and <span class="math inline">\(Y^2\)</span> are independent r.v.s.</p>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-29">Question</a>
</h4>
</div>
<div id="unnamed-chunk-29" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="kaist">
2012-Winter
</div>
Let <span class="math inline">\((\Omega, \mathcal{B}, P)\)</span> be a probability space, and let <span class="math display">\[
\mathcal{G}:=\{A\in\mathcal{B}: P\{A\} =0\text{ or } 1 \}.
\]</span> Let <span class="math inline">\(X, Y: (\Omega, \mathcal{G})\rightarrow (\mathbb{R}, \mathcal{B}(\mathbb{R}))\)</span> be two random variables. Are <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> independent?
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-30">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-30" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(My idea)</p>
<p>We need to check that every event in each variable is independent.</p>
<p>(i). when events <span class="math inline">\(\{ X \in A \}\)</span>, <span class="math inline">\(\{Y \in B\}\)</span> are both probability zero, then <span class="math display">\[
P(X\in A, Y \in B) = P(X \in A)P(Y \in B)=0.
\]</span></p>
<p>(ii). when events <span class="math inline">\(\{ X \in A \}\)</span>, <span class="math inline">\(\{Y \in B\}\)</span> are both probability one, then <span class="math display">\[
P(X\in A, Y \in B) = P(X \in A)P(Y \in B)=1.
\]</span></p>
<p>(iii). when <span class="math inline">\(P(X\in A)=1\)</span> and <span class="math inline">\(P(Y \in B)= 0\)</span>, then <span class="math inline">\(P(X \in A, Y\in B) \leq P(Y\in B) = 0\)</span>. Therefore <span class="math inline">\(P(X \in A Y\in B) = 0 = P(X\in A) P(Y \in B).\)</span></p>
Therefore, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.
</div>
</div>
</div>
</div>
<p>One of the first things to understand about the definition of independent events is that it is not enough to assume <span class="math inline">\(P(A_i \in A_j) = P(A_i)P(A_j)\)</span> for all <span class="math inline">\(i\neq j\)</span>. A sequence of events <span class="math inline">\(A_1, \ldots, A_n\)</span> with the last property is called <strong>pairwise independent</strong>. It is clear that independent events are pairwise independent. But the converse is not true.</p>
<div id="sufficient-conditions-for-independence" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Sufficient Conditions for Independence</h3>
<p>We need to a definition that generalizes all our earlier definitions.</p>
<p>Collections of sets <span class="math inline">\(\mathcal{A}_1, \ldots, \mathcal{A}_2, \ldots, \mathcal{A}_n \subset \mathcal{F}\)</span> are said to be <strong>independent</strong> if whenever <span class="math inline">\(A_i \in \mathcal{A}_i\)</span> and <span class="math inline">\(I \subset \{ 1, \ldots, n \}\)</span>, we have <span class="math inline">\(P(\cap_{i \in I}A_{i}) = \prod_{i\in I} P(A_i)\)</span>.</p>
<p>If each collection is a single set, i.e., <span class="math inline">\(\mathcal{A}_i = \{ A_i \}\)</span>, this definition reduces to the one for sets.</p>
<div class="lemma" btit="equivalent condition for independence">
<p>Without loss of generality we can suppose each <span class="math inline">\(\mathcal{A}_i\)</span> contains <span class="math inline">\(\Omega\)</span>. In this case, the condition is equivalent to <span class="math display">\[
P(\cap_{i=1}^{n}A_i) = \prod_{i=1}^{n}P(A_i) \quad{\text{whenever } A_i \in \mathcal{A}_{i}}
\]</span> since we can set <span class="math inline">\(A_i = \Omega\)</span> for <span class="math inline">\(i \notin I\)</span>.</p>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-31">Question</a>
</h4>
</div>
<div id="unnamed-chunk-31" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="kaist">
2013-Winter
</div>
If <span class="math inline">\(\{A_n : n \geq 0 \}\)</span> is an independent sequence of events, show <span class="math display">\[
P(\cap_{n=1}^{\infty}A_n) = \prod_{n=1}^{\infty}P(A_n).
\]</span>
</div>
</div>
</div>
</div>
<div class="theorem" btit="Dynkin&#39;s system and independence">
<p>Suppose <span class="math inline">\(\mathcal{A}_1 ,\ldots \mathcal{A}_2, \ldots, \mathcal{A}_n\)</span> are independent and each <span class="math inline">\(\mathcal{A}_{i}\)</span> is a <span class="math inline">\(\pi\)</span>-system. Then <span class="math inline">\(\sigma(\mathcal{A}_1), \sigma(\mathcal{A}_2), \ldots, \sigma(\mathcal{A}_n)\)</span> are independent.</p>
</div>
</div>
<div id="independence-distribution-and-expectation" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Independence, Distribution, and Expectation</h3>
<div class="theorem" btit="independence and expectation">
<p>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent and have distributions <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\nu\)</span>. If <span class="math inline">\(h:\mathbb{R}^{2} \rightarrow \mathbb{R}\)</span> is a measurable function with <span class="math inline">\(h \geq 0\)</span> pr <span class="math inline">\(E|h(X,Y)|&lt;\infty\)</span>, then <span class="math display">\[
Eh(X,Y) = \int \int h(x,y)\mu(dx)\nu(dy).
\]</span> In particular, if <span class="math inline">\(h(x,y) = f(x) g(y)\)</span> where <span class="math inline">\(f,g: \mathbb{R} \rightarrow \mathbb{R}\)</span> are measurable functions with <span class="math inline">\(f,g \geq 0\)</span> or <span class="math inline">\(E|f(X)|\)</span> and <span class="math inline">\(E|g(Y)| &lt; \infty\)</span>, then <span class="math display">\[
Ef(X)g(Y) = Ef(X)\cdot Eg(Y).
\]</span></p>
</div>
</div>
<div id="sums-of-independent-random-variables" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Sums of Independent Random Variables</h3>
<div class="theorem" btit="Convolution">
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, <span class="math inline">\(F(x) = P(X \leq x)\)</span>, and <span class="math inline">\(G(y) = P(Y \leq y)\)</span>, then <span class="math display">\[
P(X+Y\leq z) = \int F(z-y)dG(y).
\]</span></p>
</div>
<p>The integral on the right-hand side is called the <strong>convolution</strong> of <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> and is denoted <span class="math inline">\(F*G(z)\)</span>. The meaning of <span class="math inline">\(dG(y)\)</span> will be explained in the proof.</p>
<div class="proof">
<p>Let <span class="math inline">\(h(x,y) = 1_{(x+y \leq z)}.\)</span> Let <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\nu\)</span> be the probability measures with distribution functions <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span>. Since for fixed <span class="math inline">\(y\)</span>, <span class="math display">\[
\int h(x,y)\mu(dx) = \int 1_{(-\infty, z-y)]}(x) \mu(dx) = F(z-y),
\]</span> using the fact <span class="math inline">\(Eh(X,Y) = \int \int h(x,y)\mu(dx)\nu(dy)\)</span> gives <span class="math display">\[\begin{align}
P(X+Y \leq z) &amp;\leq \int \int 1_{(x+y \leq z)}\mu (dx) \nu (dy)\\
&amp;= \int F(z-y)\nu (dy) = \int F(z-y)dG(y).
\end{align}\]</span> The last equality is just a change of notation: We regard <span class="math inline">\(dG(y)\)</span> as a shorthand for “integrate with respect to the measure <span class="math inline">\(\nu\)</span> with distribution functon <span class="math inline">\(G\)</span>.”</p>
</div>
</div>
<div id="constructing-independent-random-variables" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Constructing Independent Random Variables</h3>
<p>The last question that we have to address before we can study independent random variables is: Do they exist? If we are given a finite number of distribution functions <span class="math inline">\(F_i, 1\leq i \leq n\)</span>, it is easy to construct independent random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> with <span class="math inline">\(P(X_i \leq x) =F_{i}(x)\)</span>. Let <span class="math inline">\(\Omega = \mathbb{R}^{n}, \mathcal{F} = \mathcal{R}^{n}\)</span>, <span class="math inline">\(X_{i}(\omega_1, \ldots, \omega_n) = \omega_i\)</span> (the <span class="math inline">\(i\)</span>-th coordinate of <span class="math inline">\(\omega \in \mathbb{R}^{n}\)</span>), and let <span class="math inline">\(P\)</span> the measure on <span class="math inline">\(\mathcal{R}^{n}\)</span> htat has <span class="math display">\[
P((a_1, b_1]\times (a_n, b_n]) = (F_1(b_1) - F_1(a_1))\cdots (F_n(b_n) - F_n(a_n)).
\]</span> If <span class="math inline">\(\mu_i\)</span> is the measure with distribution function <span class="math inline">\(F_i\)</span>, then <span class="math inline">\(P = \mu_1 \times \ldots \times \mu_n\)</span>.</p>
<p>To construct an infinite sequence <span class="math inline">\(X_1, X_2, \ldots\)</span> of independent random variables with given distribution functions, we want to perform the last construction on the infinite product space <span class="math display">\[
\mathbb{R}^{N} = \{(\omega_1, \omega_2, \ldots) : \omega_i \in \mathbb{R} \} = \{\text{functions } \omega: \mathbb{N} \rightarrow \mathbb{R}\}.
\]</span> We define <span class="math inline">\(X_i (\omega) = \omega_i\)</span> and we equip <span class="math inline">\(\mathbb{R}^{N}\)</span> with the product <span class="math inline">\(\sigma\)</span>-field <span class="math inline">\(\mathcal{R}^{N}\)</span>, which is generated by the <strong>finite dimensional sets</strong>=sets of the form <span class="math inline">\(\{\omega: \omega_{i} \in B_i , 1 \leq i \leq n\}\)</span>, where <span class="math inline">\(B_i \in \mathcal{R}\)</span>. It is clear how we want to define <span class="math inline">\(P\)</span> for finite dimensional sets. To assert the existence of a unique extension to <span class="math inline">\(\mathcal{R}^{N}\)</span>, we use the Kolmogorov’s extension theorem.</p>
<div class="theorem" btit="Kolmogorov&#39;s extension theorem">
<p>Suppose we are given probability measures <span class="math inline">\(\mu_n\)</span> on <span class="math inline">\(\mathbb{R}^n, \mathcal{R}^{n}\)</span> that are consistent, that is, <span class="math display">\[
\mu_{n+1}((a_1, b_1] \times \cdots \times (a_n, b_n]\times \mathbb{R}) = \mu_n ((a_1, b_1] \times \cdots \times (a_n, b_n]).
\]</span> Then there is a unique probability measure <span class="math inline">\(P\)</span> on <span class="math inline">\((\mathbb{R}^n, \mathcal{R}^{n})\)</span> with <span class="math display">\[
P(\omega: \omega_i \in (a_i, b_i], 1\leq i \leq n) = \mu_n ((a_1, b_1] \times \cdots \times (a_n, b_n]).
\]</span></p>
</div>
<ul>
<li><p><span class="math inline">\((S, \mathcal{S})\)</span> is said to be <strong>nice</strong> if there is 1-1 map <span class="math inline">\(\varphi\)</span> from <span class="math inline">\(S\)</span> into <span class="math inline">\(\mathbb{R}\)</span> so that <span class="math inline">\(\varphi\)</span> and <span class="math inline">\(\varphi^{-1}\)</span> are both measurable.</p></li>
<li><p>Such spaces are often called <strong>standard Borel spaces</strong>, but we already have too many things named after Borel. The next result shows that most spaces arising in applications are nice.</p></li>
</ul>
</div>
</div>
<div id="convergence-of-random-variables" class="section level2">
<h2><span class="header-section-number">2.2</span> Convergence of Random variables</h2>
<div id="complete-convergence" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Complete convergence</h3>
<p>First, we introduce a somewhat less common, but very useful convergence concept called <strong>complete convergence</strong>. It is also closely related to the Borel-Cantelli lemmas. <span class="citation">(Gut 2014)</span></p>
<div class="definition" btit="converge completely">
<p>(page 203 of <span class="citation">(Gut 2014)</span>) <span class="math inline">\(X_n\)</span> <strong>converges completely</strong> to the random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span> iff <span class="math display">\[
\sum_{n=1}^{\infty}P(|X_n - X|&gt;\varepsilon) &lt; \infty, \quad{\forall \varepsilon &gt; 0.}
\]</span></p>
</div>
</div>
<div id="almost-sure-convergence" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Almost sure convergence</h3>
<p>One speaks of the convergence of functions: <span class="math inline">\(f_n : \mathbb{R} \rightarrow \mathbb{R}\)</span>, then <span class="math inline">\(\lim_{n\rightarrow \infty} f_n = f\)</span> if <span class="math inline">\(\lim_{n\rightarrow \infty}f_n (x) = f(x)\)</span> for all <span class="math inline">\(x\)</span> in <span class="math inline">\(\mathbb{R}\)</span>. This is called <strong>pointwise convergence of functions</strong>. A random variable is of course a function <span class="math inline">\((X: \Omega \rightarrow \mathbb{R}\)</span> for an abstract space <span class="math inline">\(\Omega\)</span>), and thus we have the same notation: a sequence <span class="math inline">\(X_n: \Omega \rightarrow \mathbb{R}\)</span> <strong>converges pointwise</strong> to <span class="math inline">\(X\)</span> if <span class="math inline">\(\lim_{n\rightarrow \infty}X_n (\omega) = X(\omega)\)</span>, for all <span class="math inline">\(\omega \in \Omega\)</span>. <span class="citation">(Jacod and Protter 2004)</span></p>
<p>However, the problem is that this natural defintion is surprisingly useless in probability.</p>
<div class="definition" btit="pointwise convergence is useless in probability theory">
<p>(page 137 of <span class="citation">(Jacod and Protter 2004)</span>) Let <span class="math inline">\(X_n\)</span> be a i.i.d. sequence of random variables with <span class="math inline">\(P(X_n = 1)=p\)</span> and <span class="math inline">\(P(X_n = 0) = 1-p\)</span>. For example, we can imagine tossing a slightly unbalanced coin (so that <span class="math inline">\(p &gt; \frac{1}{2})\)</span> repeatedly, and <span class="math inline">\(\{X_n = 1\}\)</span> corresponds to heads on the <span class="math inline">\(n\)</span>-th toss and <span class="math inline">\(\{X_n = 0\}\)</span> corresponds to tails on the <span class="math inline">\(n\)</span>-th toss. In the “long run”, we would expect the proportion of heads to be <span class="math inline">\(p\)</span>; this would justify our model that claims the probability of heads is <span class="math inline">\(p\)</span>. Mathematically we would want <span class="math display">\[
\lim_{n\rightarrow \infty} \frac{X_1 (\omega) + \ldots + X_n(\omega)}{n} = p \quad{\text{for all } \omega \in \Omega.}
\]</span> This simply does not happen! For example, let <span class="math inline">\(\omega = \{T, T, T, \ldots, \}\)</span>, the sequence of all tails. For this <span class="math inline">\(\omega_0\)</span>, <span class="math display">\[
\lim_{n\rightarrow \infty} \frac{1}{n}\sum_{j=1}^{n}X_j (\omega_0) = 0.
\]</span> More generally, we have the event <span class="math display">\[
A =\{ \omega : \text{ only a finite number of heads occur} \} .
\]</span> Then <span class="math display">\[
\lim_{n\rightarrow \infty}\frac{1}{n} \sum_{j=1}^{n}X_j (\omega) = 0, \forall \omega \in A.
\]</span> We readily admit that the event <span class="math inline">\(A\)</span> is very unlikely to occur. Indeed, we can show that <span class="math inline">\(P(A)=0\)</span>. In fact, what we will eventually show is that <span class="math display">\[
P\Bigg( \Big\{ \omega : \lim_{n\rightarrow \infty} \frac{1}{n}\sum_{j=1}^{n}X_j(\omega) = p \Big\}\Bigg) =1.
\]</span> This type of convergence of random variables, where we do not have convergence for <em>all</em> <span class="math inline">\(\omega\)</span> but do have convergence for <em>almost all</em> <span class="math inline">\(\omega\)</span> (i.e., the set of <span class="math inline">\(\omega\)</span> where we do have convergence has probability one), is what typically arises.</p>
</div>
<div class="definition" btit="converge in almost surely">
<p>(page 14 of <span class="citation">(Durrett 2019)</span>) Note that <span class="math display">\[
\Omega_{0} \equiv \{ \omega : \lim_{n\rightarrow \infty}X_n \text{ exists}\} = \{ \omega : \text{lim sup}_{n\rightarrow \infty} X_n - \text{lim inf}_{n\rightarrow \infty}X_n = 0.\}
\]</span> is a measurable set. If <span class="math inline">\(P(\Omega_0)=1\)</span>, we say that <span class="math inline">\(X_n\)</span> <strong>converges almost surly</strong>,, or a.s. for short. This type of convergence is called <strong>almost everywhere</strong> in measure theory.</p>
</div>
<p>To have a limit defined on the whole space, it is convenient to let <span class="math display">\[
X_{\infty} = \text{lim sup}_{n\rightarrow \infty}X_n.
\]</span> Note that this random variable may take the value <span class="math inline">\(+\infty\)</span> or <span class="math inline">\(-\infty\)</span>.</p>
<p>Just as we defined almost sure convergence because it naturally occurs when “pointwise convergence” (for all “points”) fails, we need to introduce twe more types of convergence. These next two types of convergence also arise naturally when a.s. convergence fails, and they are also useful as tools to help to show that a.s. convergence holds. <span class="citation">(Jacod and Protter 2004)</span></p>
</div>
<div id="lp-convergence" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Lp Convergence</h3>
<div class="definition" btit="converge in $L^p$">
<p>(page 138 of <span class="citation">(Jacod and Protter 2004)</span>) A sequence of random variables <span class="math inline">\((X_n)_{n\geq 1}\)</span> converges in <span class="math inline">\(L^p\)</span> to <span class="math inline">\(X (1\leq p &lt; \infty)\)</span> if <span class="math inline">\(|X_n|\)</span>, <span class="math inline">\(|X|\)</span> are in <span class="math inline">\(L^{p}\)</span> and: <span class="math display">\[
\lim_{n\rightarrow \infty} E\{ |X_n - X|^{p} \}=0.
\]</span> Alternatively one says <span class="math inline">\(X_n\)</span> converges to <span class="math inline">\(X\)</span> in <span class="math inline">\(p\)</span>-th mean, and one writes <span class="math display">\[
X_n \stackrel{L^{p}}{\rightarrow}X.
\]</span></p>
</div>
<p>Note that when <span class="math inline">\(X_n \stackrel{L^{p}}{\rightarrow}X\)</span>, for <span class="math inline">\(p\in (1,\infty)\)</span>, we have that <span class="math inline">\(E\{|X_n|^{p}\}\)</span> converges to <span class="math inline">\(E\{|X|^{p}\}\)</span>. <span class="citation">(Jacod and Protter 2004)</span></p>
<p>When <span class="math inline">\(p=1\)</span>, we have <span class="math inline">\(|E\{X_n - X\}| \leq E\{|X_n - X|\}\)</span> and <span class="math inline">\(|E\{|X_n|\} - E\{|X|\}\leq E\{|X_n - X|\}\)</span> because <span class="math inline">\(||x|-|y|| \leq |x-y|\)</span>. Hence <span class="math inline">\(X_n \stackrel{L^1}{\rightarrow}X\)</span> implies <span class="math inline">\(E\{X_n \} \rightarrow E\{X\}\)</span> and <span class="math inline">\(E\{|X_n|\} \rightarrow E\{|X|\}\)</span>. <span class="citation">(Jacod and Protter 2004)</span></p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-32">Question</a>
</h4>
</div>
<div id="unnamed-chunk-32" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2008-Winter
</div>
State <span class="math inline">\(L_{1}\)</span> convergence.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-33">Question</a>
</h4>
</div>
<div id="unnamed-chunk-33" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
(page 145, Exercise 17.14 of <span class="citation">(Jacod and Protter 2004)</span>) Let <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span> be real-valued r.v.’s in <span class="math inline">\(L^2\)</span>, and suppose that <span class="math inline">\(X_n\)</span> tends to <span class="math inline">\(X\)</span> in <span class="math inline">\(L^2\)</span>. Show that <span class="math inline">\(E\{X_n^2\}\)</span> tends to <span class="math inline">\(E\{X^2\}\)</span>. (Hint: use that <span class="math inline">\(|x^2 - y^2| = (x-y)^2 +2|y||x-y|\)</span> and the Cauchy-Schwarz inequality).
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-34">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-34" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
First observe that <span class="math inline">\(|E(X_n^2) -E(X^2)| \leq E(|X_n^2 - X^2|)\)</span>. Since <span class="math inline">\(|X_n^2 - X^2| \leq (X_n - X)^2 + 2|X| |X_n - X\)</span>, we get that <span class="math inline">\(|E(X_n^2) - E(X^2) | \leq E((X_n - X)^2) + 2E(|X||X_n - X|)\)</span>. Note that first term goes to <span class="math inline">\(0\)</span> since <span class="math inline">\(X_n\)</span> tends to <span class="math inline">\(X\)</span> in <span class="math inline">\(L^2\)</span>. Applying Cauchy-Schwarz inequality to the second trem, we get <span class="math inline">\(E(|X||X_n - X|) \leq \sqrt{E(X^2)E(|X_n - X|^2)}\)</span>, hence the seconde term also goes to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>. Not we can conclude <span class="math inline">\(E(X_n^2) \rightarrow E(X^2)\)</span>.
</div>
</div>
</div>
</div>
</div>
<div id="converge-in-probability" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Converge in Probability</h3>
<div class="definition" btit="converge in probability">
<p><span class="math inline">\(Y_n\)</span> converges to <span class="math inline">\(Y\)</span> <strong>in probability</strong> if for all <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math inline">\(P(|Y_n - Y| &gt; \epsilon) \rightarrow 0\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
</div>
<p>Before we establish the relationships between the different types of convergence, we give a surprisingly useful small result which characterizes convergence in probability. <span class="citation">(Jacod and Protter 2004)</span></p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-35">Question</a>
</h4>
</div>
<div id="unnamed-chunk-35" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2002-Winter
</div>
<div class="snu">
2005-Winter
</div>
<div class="snu">
2008-Winter
</div>
<div class="snu">
2009-Summer
</div>
<div class="snu">
2011-Summer
</div>
<div id="Theorem17.1" class="theorem">
<p>(<span class="citation">Durrett (2019)</span>, Exercise 2.3.6.)</p>
<p>Prove that <span class="math inline">\(X_n \rightarrow X\)</span> in probability if and only if <span class="math inline">\(E \frac{|X_n - X|}{1 + |X_n - X|} \rightarrow 0\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-36">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-36" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Jacod and Protter (2004)</span>, Theorem 17.1)</p>
<p>There is no loss of generality by taking <span class="math inline">\(X= 0\)</span>. Thus we want to show <span class="math inline">\(X_n \stackrel{P}{\rightarrow} 0\)</span> if and only if <span class="math inline">\(\lim_{n\rightarrow \infty} E\{\frac{|X_n|}{1+|X_n|}\} = 0\)</span>. First suppose that <span class="math inline">\(X_n \stackrel{P}{\rightarrow} 0\)</span>. Then for any <span class="math inline">\(\varepsilon &gt;0\)</span>, <span class="math inline">\(\lim_{n\rightarrow \infty} P(|X_n|&gt; \epsilon) = 0\)</span>. Note that <span class="math display">\[
\frac{|X_n|}{1+|X_n|} \leq \frac{|X_n|}{1+|X_n|}1_{\{ |X_n| &gt;\varepsilon \}} + \varepsilon 1_{\{ |X_n| \leq \varepsilon \}} \leq 1_{\{ |X_n | &gt; \varepsilon \}} + \varepsilon.
\]</span> Therefore <span class="math display">\[
E \Big\{ \frac{|X_n|}{1+|X_n|}  \Big\} \leq E\{1_{\{ |X_n| &gt; \varepsilon\}}\} + \varepsilon = P(|X_n| &gt; \varepsilon) + \varepsilon.
\]</span> Taking limits yields <span class="math display">\[
\lim_{n\rightarrow\infty}E\Big\{ \frac{|X_n|}{1+|X_n|} \Big\} \leq \varepsilon;
\]</span> since <span class="math inline">\(\varepsilon\)</span> was arbitrary we have <span class="math inline">\(\lim_{n\rightarrow \infty} E \{ \frac{|X_n|}{1+|X_n|}\}=0\)</span>.</p>
<p>Next suppose <span class="math inline">\(\lim_{n\rightarrow \infty} E \{ \frac{|X_n|}{1+|X_n|}\}=0\)</span>. The function <span class="math inline">\(f(x) = \frac{x}{1+x}\)</span> has derivative <span class="math inline">\(\frac{1}{(1+x)^2} &gt; 0\)</span> and is increasing. Therefore <span class="math display">\[
\frac{\varepsilon}{1+\varepsilon} \lim_{n\rightarrow \infty} P(|X_n| &gt; \epsilon) \leq \lim_{n\rightarrow \infty} E \Big\{  \frac{|X_n|}{1+|X_n|} \Big\} = 0.
\]</span> Since <span class="math inline">\(\varepsilon &gt;0\)</span> is fixed, we conclude <span class="math inline">\(\lim_{n\rightarrow \infty} P(|X_n|&gt;\epsilon)=0\)</span>.</p>
<div class="remark">
<p>What this theorem say is that <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span> iff <span class="math inline">\(E\{f(|X_n - X|) \}\rightarrow 0\)</span> for the function <span class="math inline">\(f(x) = \frac{|x|}{1+|x|}\)</span>. A careful examination of the proof shows that the same equivalence holds for any function <span class="math inline">\(f\)</span> on <span class="math inline">\(\mathbb{R}_{+}\)</span> which is bounded, strictly increasing on <span class="math inline">\((0,1)\)</span>, continuous, and with <span class="math inline">\(f(0) = 0\)</span>. For example we have <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span> iff <span class="math inline">\(E\{|X_n - X| \wedge 1\} \rightarrow 0\)</span> and also iff <span class="math inline">\(E\{\text{arctan}(|X_n - X|)\}\rightarrow 0\)</span>. (<span class="math inline">\(\wedge\)</span> means a minimum) <span class="citation">(Jacod and Protter 2004)</span></p>
</div>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-37">Question</a>
</h4>
</div>
<div id="unnamed-chunk-37" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2009-Winter
</div>
Prove that if <span class="math inline">\(X_n\rightarrow X\)</span> a.s., <span class="math inline">\(E \frac{|X_n - X|}{1 + |X_n - X|} \rightarrow 0\)</span>.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-38">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-38" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(My idea)</p>
Since <span class="math inline">\(X_n\rightarrow X\)</span> a.s. implies, <span class="math inline">\(X_n \rightarrow X\)</span> in probability, we can prove it by following the above proof.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-39">Question</a>
</h4>
</div>
<div id="unnamed-chunk-39" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2008-Summer
</div>
<div class="snu">
2013-Winter
</div>
<div class="snu">
2017-Winter
</div>
Suppose that <span class="math inline">\(X_n \rightarrow X\)</span> in probability, and let <span class="math inline">\(Y = \sup_{n}|X_n-X|\)</span>. Define a measure <span class="math inline">\(Q\)</span> such that <span class="math display">\[
Q(A) =\frac{1}{c}E\{I_{A}\frac{1}{1+Y} \} \quad{\text{ where }c = E\frac{1}{1+Y}}.
\]</span> Show that <span class="math inline">\(E|X_n - X| \rightarrow 0\)</span> under <span class="math inline">\(Q\)</span>.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-40">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-40" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Jacod and Protter (2004)</span>, Exercise 17.12)</p>
<p>Before solving this problem, we need to prove</p>
<div class="proposition" btit="Exercise 17.11 of Jacod 2004">
<p>Suppose <span class="math inline">\(\lim_{n\rightarrow \infty} X_n = X\)</span> a.s. and <span class="math inline">\(X &lt; \infty\)</span> a.s. Let <span class="math inline">\(Y = \sup_n |X_n|\)</span>. Show that <span class="math inline">\(Y&lt; \infty\)</span> a.s.</p>
</div>
<div class="proposition" btit="Exercise 9.7 of Jacod 2004">
<p>Given <span class="math inline">\((\Omega, \mathcal{A}, P)\)</span>, suppose <span class="math inline">\(X\)</span> is a r.v. with <span class="math inline">\(X \geq 0\)</span> a.s. and <span class="math inline">\(E\{X\} = 1\)</span>. Define <span class="math inline">\(Q : \mathcal{A} \rightarrow \mathbb{R}\)</span> by <span class="math inline">\(Q(A) = E\{X 1_{A} \}\)</span>. Suppose also <span class="math inline">\(P(X&gt;0) = 1\)</span>. Let <span class="math inline">\(E_Q\)</span> denote expectation with respect to <span class="math inline">\(Q\)</span>. Show that <span class="math inline">\(E_{Q}\{Y\} = E_{P}\{XY\}\)</span>.</p>
</div>
<div class="proof">
<ul>
<li><p>Let’s prove this first for simple functions, i.e., let <span class="math inline">\(Y\)</span> be of the form <span class="math display">\[
Y = \sum_{i=1}^{n}c_i \mathbf{1}_{A_i}
\]</span> for disjoint <span class="math inline">\(A_1 ,\ldots, A_n\)</span>. Then <span class="math display">\[
E_{Q}[Y] = \sum_{i=1}^{n}c_{i}Q(A_i) = \sum_{i=1}^{n}c_{i}E[X\mathbf{1}_{A_i}] = E_{P}[XY].
\]</span></p></li>
<li><p>For non-negative <span class="math inline">\(Y\)</span> we take a sequence of simple functions <span class="math inline">\(Y_n \uparrow Y\)</span>. Then <span class="math display">\[
E_{Q}[Y] = \lim_{n\rightarrow \infty}E_{Q}[Y_n] = \lim_{n\rightarrow \infty} E_{P}[XY_n] = E_{P}[XY],
\]</span> where the last equality follows from the monotone convergence theorem.</p></li>
<li><p>For general <span class="math inline">\(Y \in L^{1}(Q)\)</span>, we have that <span class="math inline">\(E_{Q}[Y] = E_{Q}[Y^{+}]-E_{Q}[Y^{-}] = E_{P}[(XY)^{-}] = E_{P}[XY]\)</span>.</p></li>
</ul>
</div>
<div class="proposition" btit="Exercise 9.8 of Jacod 2004">
<p>Given <span class="math inline">\((\Omega, \mathcal{A}, P)\)</span>, suppose <span class="math inline">\(X\)</span> is a r.v. with <span class="math inline">\(X \geq 0\)</span> a.s. and <span class="math inline">\(E\{X\} = 1\)</span>. Define <span class="math inline">\(Q : \mathcal{A} \rightarrow \mathbb{R}\)</span> by <span class="math inline">\(Q(A) = E\{X 1_{A} \}\)</span>.</p>
<p>(a). Show that $ is integrable for <span class="math inline">\(Q\)</span>.</p>
<p>(b). Define <span class="math inline">\(R: \mathcal{A} \rightarrow \mathbb{R}\)</span> by <span class="math inline">\(R(A) = E_{Q} \{\frac{1}{X} 1_{A} \}\)</span>. Show that <span class="math inline">\(R\)</span> is exactly the probability measure <span class="math inline">\(P\)</span>.</p>
</div>
<div class="proof">
<p>(a). Note that <span class="math inline">\(\frac{1}{X}X=1\)</span> a.s. since <span class="math inline">\(P(X&gt;0) = 1\)</span>. By problem 9.7, <span class="math inline">\(E_{Q}[\frac{1}{X}\mathbf{1}_{A}]\)</span> we have <span class="math inline">\(Q(A) = 0 \Rightarrow P(A) = 0\)</span>. Now combining the results of the previous problems we can easily observe that <span class="math inline">\(Q(A) = 0 \Leftrightarrow P(A) = 0\)</span> iff <span class="math inline">\(P(X&gt;0)= 1\)</span>.</p>
</div>
<p><span class="math inline">\(Y&lt;\infty\)</span> a.s. which follows by Exercise 17.11 since <span class="math inline">\(X_n &lt; \infty\)</span> and <span class="math inline">\(X &lt;\infty\)</span> a.s. Let <span class="math inline">\(Z = \frac{1}{c}\frac{1}{1+Y}\)</span>. Observe that <span class="math inline">\(Z &gt; 0\)</span> a.s. and <span class="math inline">\(E_{P}(Z) =1\)</span>. Therefore as in Exercise 9.8 <span class="math inline">\(Q(A) = E_{P}(Z\mathbf{1}_A)\)</span> defines a probability measure and <span class="math inline">\(E_{Q}(|X_n - X|) = E_{P}(Z|X_n -X|)\)</span>. Note that <span class="math inline">\(Z|X_n - X|\leq 1\)</span> a.s. and <span class="math inline">\(X_n \rightarrow X\)</span> a.s. by hypothesis, hence by dominated convergence theorem, <span class="math inline">\(E_{Q}(|X_n - X|) = E_{P}(Z|X_n - X|) \rightarrow 0\)</span>, i.e. <span class="math inline">\(X_n\)</span> tends to <span class="math inline">\(X\)</span> in <span class="math inline">\(L^1\)</span> with respect to <span class="math inline">\(Q\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<div id="relationship-between-convergence-concepts" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Relationship between Convergence Concepts</h3>
<p>A natural first problem is to determine the hierarchy between the convergence concepts. <span class="citation">(Gut 2014)</span></p>
<div class="theorem" btit="Theorem 5.3.1 of Gut 2004">
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(X_1, X_2, \ldots\)</span> be random variables. The following implications hold as <span class="math inline">\(n \rightarrow \infty\)</span>: <span class="math display">\[\begin{align}
X_n \stackrel{\text{c.c.}}{\rightarrow} X \Rightarrow X_n \stackrel{a.s.}{\rightarrow} X \Rightarrow X_n &amp;\stackrel{p}{\rightarrow}X \Rightarrow X_n \stackrel{d}{\rightarrow} X\\
&amp;\Uparrow\\
X_n &amp;\stackrel{L^{p}}{\rightarrow}X
\end{align}\]</span> All implications are strict.</p>
</div>
<p>The next theorem shows that convergence in probability is the weakest of the three types of convergence (a.s., <span class="math inline">\(L^p\)</span> and probability).</p>
<div class="theorem" btit="Theorem 17.2 of Jacod 2004">
<p>Let <span class="math inline">\((X_n)_{n\geq 1}\)</span> be a sequence of random variables.</p>
<p>(a). If <span class="math inline">\(X_n \stackrel{L^{p}}{\rightarrow}X\)</span>, then <span class="math inline">\(X_n \stackrel{p}{\rightarrow}{X}\)</span>.</p>
<p>(b). If <span class="math inline">\(X_n \stackrel{\text{a.s.}}{\rightarrow}X\)</span>, then <span class="math inline">\(X_n \stackrel{p}{\rightarrow}{X}\)</span>.</p>
</div>
<div class="proof">
<p>(a). Recall that for an event <span class="math inline">\(A\)</span>, <span class="math inline">\(P(A) = E\{1_A\}\)</span>, where <span class="math inline">\(1_{A}\)</span> is the indicator function of the event <span class="math inline">\(A\)</span>. Therefore, <span class="math display">\[
P\{|X_n - X|&gt;\varepsilon\} = E\{1_{\{|X_n - X|&gt;\varepsilon \}} \}.
\]</span> Note that <span class="math inline">\(\frac{|X_n - X|^{p}}{\varepsilon^p}&gt;1\)</span> on the event <span class="math inline">\(\{ |X_n - X| &gt; \varepsilon\}\)</span>, hence <span class="math display">\[\begin{align}
&amp;\leq E \Big\{ \frac{|X_n - X|^p}{\varepsilon^p} 1_{\{ |X_n - X| &gt; \varepsilon \}} \Big\}
\\
&amp;= \frac{1}{\varepsilon^p} E\{ |X_n - X|^{p}1_{\{|X_n - X| &gt; \varepsilon\}} \},
\end{align}\]</span> and since <span class="math inline">\(|X_n - X|^{p} \geq 0\)</span> always, we can simply drop the indicator function to get: <span class="math display">\[
\leq \frac{1}{\varepsilon^p} E\{ |X_n - X|^{p} \}.
\]</span> The last expression tends to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\)</span> tends to <span class="math inline">\(\infty\)</span> (for fixed <span class="math inline">\(\varepsilon &gt;0\)</span>), which gives the result.</p>
<p>(b). Since <span class="math inline">\(\frac{|X_n - X|}{1 + |X_n - X|} \leq 1\)</span> always, we have <span class="math display">\[
\lim_{n\rightarrow \infty}E\Big\{ \frac{|X_n - X|}{1 + |X_n - X|} \Big\}=E\Big\{\lim_{n\rightarrow \infty} \frac{|X_n - X|}{1 + |X_n - X|} \Big\} = E\{0\} = 0
\]</span> by Lebesgue’s Dominated Convergence Theorem. We then apply Theorem 17.1 of Jacod 2004.</p>
</div>
<div class="theorem" btit="Theorem 17.3 of Jacod 2004">
<p>Suppose <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span>. Then there exists a subsequence <span class="math inline">\(n_k\)</span> such that <span class="math inline">\(\lim_{k\rightarrow \infty} X_{n_k} = X\)</span> almost surely.</p>
</div>
<div class="theorem" btit="Theorem 17.4 of Jacod 2004">
<p>Suppose <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span> and also that <span class="math inline">\(|X_n | \leq Y\)</span>, all <span class="math inline">\(n\)</span>, and <span class="math inline">\(Y \in L^{p}\)</span>. Then <span class="math inline">\(|X|\)</span> is in <span class="math inline">\(L^{p}\)</span> and <span class="math inline">\(X_n \stackrel{L^{p}}{\rightarrow}X\)</span>.</p>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-41">Question</a>
</h4>
</div>
<div id="unnamed-chunk-41" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="kaist">
2014-Winter
</div>
<div class="kaist">
2016-Winter
</div>
<div class="kaist">
2020-Winter
</div>
<p><span class="math inline">\(\{X_n\}\)</span> is said to converge completely to <span class="math inline">\(X\)</span> if <span class="math inline">\(\sum_{n} P(|X_n - X|&gt; \epsilon) &lt; \infty\)</span> for every &gt;0$. Prove or disprove:</p>
<p>(a). If <span class="math inline">\(\{X_n\}\)</span> converges completely to <span class="math inline">\(X\)</span> then it converges a.s. to <span class="math inline">\(X\)</span>.</p>
(b). If <span class="math inline">\(\{X_n\}\)</span> converges a.s. to <span class="math inline">\(X\)</span> then it converges completely to <span class="math inline">\(X\)</span>.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-42">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-42" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(Page 209 of <span class="citation">Gut (2014)</span>)</p>
<p>(a). Immediate from the first Borel-Cantelli lemma.</p>
<p><a href="https://stats.stackexchange.com/questions/141219/almost-sure-convergence-does-not-imply-complete-convergence">reference</a></p>
<p>(b). Let <span class="math inline">\(\Omega = (0,1)\)</span> with the Borel <span class="math inline">\(\sigma\)</span>-algebra <span class="math inline">\(\mathcal{F}\)</span> and uniform measure <span class="math inline">\(\mu\)</span>. Define <span class="math display">\[
X_n (\omega) = 2 +(-1)^{n} \quad{ \text{ when } \omega \leq \frac{1}{n}}
\]</span> and <span class="math inline">\(X_n(\omega) = 0\)</span> otherwise. The <span class="math inline">\(X_n\)</span> are obviously measurable on the probability space <span class="math inline">\((\Omega, \mathcal{F}, \mu)\)</span>.</p>
<p>For any <span class="math inline">\(\omega \in \Omega\)</span> and all <span class="math inline">\(N&gt; \frac{1}{\omega}\)</span> it is the case that <span class="math inline">\(X_n(\omega) = 0\)</span>. Thus, by definition, the sequence <span class="math inline">\((X_n)\)</span> converges to 0 (not just almost surely!)</p>
<div class="figure">
<img src="images/olIjt.png" alt="" />
<p class="caption">Sequence X_n.</p>
</div>
<p>However, whenever <span class="math inline">\(0&lt;\epsilon &lt; 1\)</span>, <span class="math inline">\(P(X_n &gt; \epsilon) = P(X_n \neq 0) =\frac{1}{n}\)</span>, whence <span class="math display">\[
\sum_{n=1}^{\infty}Pr(X_n &gt; \epsilon) = \sum_{n=1}^{\infty}\frac{1}{n},
\]</span> which diverges to <span class="math inline">\(\infty\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<div id="converses" class="section level3">
<h3><span class="header-section-number">2.2.6</span> Converses</h3>
<p>Under certain additional assumptions, converses to some of the arrows. In the first two cases to follow we require the limit <span class="math inline">\(X\)</span> to be degenerate, that is, that <span class="math inline">\(P(X = c) = 1\)</span> for some constant <span class="math inline">\(c\)</span>. <span class="citation">(Gut 2014)</span></p>
<div class="theorem" btit="Theorem 5.3.2 of Gut 2014">
<p>If <span class="math inline">\(X_1, X_2, \ldots\)</span> are independent and <span class="math inline">\(c\)</span> a constant, then <span class="math display">\[
X_n \stackrel{\text{c.c.}}{\rightarrow} c \Leftrightarrow X_n \stackrel{\text{a.s.}}{\rightarrow} c \text{ as } n \rightarrow \infty.
\]</span></p>
</div>
<div class="theorem" btit="Theorem 5.3.3 of Gut 2014">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be random variables and <span class="math inline">\(c\)</span> a constant. Then <span class="math display">\[
X_n \stackrel{d}{\rightarrow}\delta(c) \text{ as } n\rightarrow\infty \Leftrightarrow X_n \stackrel{p}{\rightarrow} c \text{ as } n \rightarrow \infty.
\]</span></p>
</div>
<p>Another kind of partial converse runs as follows.</p>
<div class="theorem" btit="Theorem 5.3.4 of Gut 2014">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be random variables such that <span class="math inline">\(X_n \stackrel{p}{\rightarrow} X\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>. Then there exists a non-decreasing subsequence <span class="math inline">\(\{n_k , k\geq 1\}\)</span> of the positive integers, such that <span class="math display">\[
X_{n_k} \stackrel{\text{c.c.}}{\rightarrow} X \text{ as } n\rightarrow \infty,
\]</span> in particular, <span class="math display">\[
X_{n_k} \stackrel{\text{a.s.}}{\rightarrow} X \text{ as } n\rightarrow \infty,
\]</span></p>
</div>
<div class="proof">
<p>By assumption there exists a non-decreasing subsequence, <span class="math inline">\(\{ n_k , k \geq 1 \}\)</span>, such that <span class="math display">\[
P(|X_{n_k} - X| &gt; \frac{1}{2^k}) &lt;\frac{1}{2^k}.
\]</span> Consequently, <span class="math display">\[
\sum_{k=1}^{\infty}P(|X_{n_k} - X| &gt; \frac{1}{2^k}) &lt;\infty.
\]</span> Since <span class="math inline">\(\frac{1}{2^k} &lt; \varepsilon\)</span> for any <span class="math inline">\(\varepsilon &gt;0\)</span> whenever <span class="math inline">\(k &gt; \log(1/\varepsilon)/\log 2\)</span> it follows that <span class="math display">\[
\sum_{k=1}^{\infty}P(|X_{n_k} - X| &gt; \varepsilon) &lt;\infty
\]</span> which proves complete convergence.</p>
</div>
</div>
</div>
<div id="uniform-integrability" class="section level2">
<h2><span class="header-section-number">2.3</span> Uniform Integrability</h2>
<p>Knowing that convergence in probability does not necessarily imply mean convergence, a natural question is whether there exist conditions that guarantee that a sequence that converges in probability (or almost surely or in distribution) also converges in <span class="math inline">\(p\)</span>-mean. It turns out that <strong>uniform integrability</strong> is the adequate concept for this problem. <span class="citation">(Gut 2014)</span></p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-43">Question</a>
</h4>
</div>
<div id="unnamed-chunk-43" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2007-Summer
</div>
State uniform integrability.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-44">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-44" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="definition" btit="uniformly integrable">
<p>(page 213, Definition 5.4.1 of <span class="citation">(Gut 2014)</span>) A sequence <span class="math inline">\(X_1, X_2, \ldots\)</span> is called <strong>uniformly integrable</strong> iff <span class="math display">\[
E|X_n|I\{ |X_n| &gt; a \} \rightarrow 0 \text{ as } a\rightarrow \infty \quad{\text{ uniformly in }n .}
\]</span> Another, equivalent, way to express uniform integrability is via the distribution func- tion; <span class="math inline">\(X_1, X_2, \ldots\)</span> is uniformly integrable iff <span class="math display">\[
\int_{|x|&gt;a} |x|dF_{X_n}(x) \rightarrow 0 \text{ as } a \rightarrow \infty \text{ uniformly in }n.
\]</span></p>
</div>
</div>
</div>
</div>
</div>
<div class="remark">
<p>The assumption that <span class="math inline">\(X_1, X_2, \ldots\)</span> have finite mean, implies taht <span class="math inline">\(E|X_n| I \{ |X_n| &gt; a\} \rightarrow 0\)</span> as <span class="math inline">\(a\rightarrow \infty\)</span> for every <span class="math inline">\(n\)</span>; the tails of convergent integrals converge to <span class="math inline">\(0\)</span>. The requirement that the sequence is uniformly integrable means that the contributions in the tails of the integrals tend to <span class="math inline">\(0\)</span> <em>uniformly</em> for all members of the sequence. <span class="citation">(Gut 2014)</span></p>
</div>
<p>Since for a uniformly integrable sequence of random variables, <span class="math display">\[
E|X_n| = E|X_n|I\{ |X_n| \leq a \} + E|X_n|I\{|X_n|&gt;a\} \leq a +1,
\]</span> for <span class="math inline">\(a\)</span> large enough, it follows immediately that the moments are uniformly bounded. However, uniform integrability is more , which is illustrated by the following theorem.</p>
<div class="theorem" btit="Theorem 5.4.1 of Gut 2014">
<p>The random variables <span class="math inline">\(X_1, X_2, \ldots\)</span> are uniformly integrable iff</p>
<p>(i). <span class="math inline">\(\sup_{n}E|X_n| &lt; \infty\)</span>;</p>
<p>(ii). for any <span class="math inline">\(\varepsilon &gt;0\)</span>, there exists <span class="math inline">\(\delta &gt;0\)</span>, such that for any set <span class="math inline">\(A\)</span> with <span class="math inline">\(P(A)&lt;\delta\)</span>, <span class="math display">\[
E|X_n|I\{A\} &lt; \varepsilon \quad{\text{uniformly in }n.}
\]</span></p>
</div>
<p>It may be difficult at times to verify uniform integrability directly. Following are some convenient sufficient criteria.</p>
<div class="theorem" btit="Theorem 5.4.2 of Gut 2014">
<p>The random variables <span class="math inline">\(X_1, X_2, \ldots\)</span> be random variables, and suppose that <span class="math display">\[
\sup_n E|X_n|^{p} &lt;\infty \quad{\text{for some } p&gt;1.}
\]</span> Then <span class="math inline">\(\{ X_n, n\geq 1 \}\)</span> is uniformly integrable. In particular this is the case if <span class="math inline">\(\{ |X_n|^{p} , n \geq 1\}\)</span> is uniformly integrable for some <span class="math inline">\(p&gt;1\)</span>.</p>
</div>
<p>With a little bit more effort one can prove the following generalization.</p>
<div class="theorem" btit="Theorem 5.4.3 of Gut 2014">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be random variables and <span class="math inline">\(g\)</span> a non-negative increasing function such that <span class="math inline">\(\frac{g(x)}{x}\rightarrow \infty\)</span> as <span class="math inline">\(x\rightarrow \infty\)</span>. If <span class="math display">\[
\sup_{n}Eg(X_n)&lt;\infty,
\]</span> then <span class="math inline">\(\{ X_n, n \geq 1 \}\)</span> is uniformly integrable.</p>
</div>
</div>
<div id="convergence-of-moments" class="section level2">
<h2><span class="header-section-number">2.4</span> Convergence of Moments</h2>
<p>We are now in the position to show that uniform integrability is the “correct” concept, that is, that a sequence that converges almost surely, in probability, or in distribution, and is uniformly integrable, converges in the mean, that moments converge and that uniform integrability is the minimal additional assumption for this to happen.</p>
</div>
<div id="convergence-of-sum-of-sequences" class="section level2">
<h2><span class="header-section-number">2.5</span> Convergence of Sum of Sequences</h2>
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> and <span class="math inline">\(Y_1, Y_2, \ldots\)</span> be sequences of random variables. Suppose taht <span class="math inline">\(X_n \rightarrow X\)</span> and that <span class="math inline">\(Y_n \rightarrow Y\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span> in some sense. To what extent can we conclude that <span class="math inline">\(X_n + Y_n \rightarrow X+Y\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>? <span class="citation">(Gut 2014)</span></p>
<div class="theorem" btit="Theorem 5.11.1 of Gut 2014">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> and <span class="math inline">\(Y_1, Y_2,\ldots\)</span> be sequences of random variables such that <span class="math display">\[
X_n \rightarrow X \text{ and } Y_n \rightarrow Y \text{ as } n \rightarrow \infty,
\]</span> completely, almost surely, in probability or in <span class="math inline">\(p\)</span>-mean, respectively. Then <span class="math display">\[
X_n + Y_n \rightarrow X + Y \text{ as } n\rightarrow \infty,
\]</span> completely, almost surely, in probability or in <span class="math inline">\(p\)</span>-mean, respectively.</p>
</div>
</div>
<div id="cauchy-convergence" class="section level2">
<h2><span class="header-section-number">2.6</span> Cauchy Convergence</h2>
<p>For a sequence of real numbers it is well known that convergence is equivalent to <strong>Cauchy convergence</strong>, viz., for a sequence <span class="math inline">\(\{a_n , n \geq 1\}\)</span> of reals, <span class="math display">\[
a_n \rightarrow a \text{ as } n \rightarrow \infty \Leftrightarrow a_n - a_m \rightarrow 0 \text{ as } n, m \rightarrow \infty.
\]</span> The fancy terminology is that <span class="math inline">\((\mathbb{R}, \mathcal{R})\)</span>, that is, the space of reals, together with its <span class="math inline">\(\sigma\)</span>-algebra of Borel sets is <strong>complete</strong>. <span class="citation">(Gut 2014)</span></p>
<p>We begin by noticing that convergence always implies Cauchy convergence via some kind of triangle inequality, together with the results in Convergence of Sum of Sequences section. For example, if <span class="math inline">\(X_n \stackrel{p}{\rightarrow}X\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>, then <span class="math display">\[
|X_n - X_m| \leq |X_n - X| + |X - X_m| \stackrel{p}{\rightarrow}0 \text{ as }n,m\rightarrow \infty,
\]</span> by Theorem 5.11.1 of <span class="citation">Gut (2014)</span>, and for distributional convergence, <span class="math display">\[
\begin{align}
|F_{X_n}(x) - F_{X_m}(x)| &amp;\leq |F_{X_n}(x) - F_{X}(x) | + F_{X}(x) - F_{X_m}(x)|\\
&amp;\rightarrow 0 \text{ as } n,m \rightarrow \infty, \text{ for }x \in C(F_{X}).
\end{align}
\]</span></p>
<p>Now, let us turn to the converses. We thus assume that our sequence is Cauchy convergent.</p>
<div id="when-cauchy-convergence-implies-almost-sure-convergence" class="section level3">
<h3><span class="header-section-number">2.6.1</span> When Cauchy convergence implies Almost Sure Convergence</h3>
<p>For a.s. convergence this follows from the corresponding result for real numbers, since the assumption is that <span class="math inline">\(\{X_n (\omega), n\geq 1\}\)</span> is Cauchy convergent for almost all <span class="math inline">\(\omega\)</span>.</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-45">Question</a>
</h4>
</div>
<div id="unnamed-chunk-45" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2005-Winter
</div>
Let <span class="math inline">\(\mathcal{L}_{2}\)</span> denote the class of r.v.’s <span class="math inline">\(X\)</span> with <span class="math inline">\(EX^{2}&lt;\infty\)</span>. Show that if <span class="math inline">\(X_n \in \mathcal{L}_{2}\)</span> and <span class="math inline">\(E|X_m - X_n|^{2}\rightarrow 0\)</span> as <span class="math inline">\(m,n\rightarrow 0\)</span>, there exists a r.v. <span class="math inline">\(X \in \mathcal{L}_{2}\)</span> such that <span class="math inline">\(E|X_n - X|^{2} \rightarrow 0\)</span> and <span class="math inline">\(X_n \rightarrow X\)</span> a.s.
</div>
</div>
</div>
</div>
</div>
<div id="when-cauchy-convergence-implies-convergence-in-probability" class="section level3">
<h3><span class="header-section-number">2.6.2</span> When Cauchy convergence implies Convergence in Probability</h3>
<p>By modifying the proof of Theorem 5.3.4 of <span class="citation">Gut (2014)</span>, one shows that there exists a subsequence, <span class="math inline">\(\{X_{n_k}, k\geq 1\}\)</span>, that is almost surely Cauchy convergent, from which we conclude that there exists a limiting random variable <span class="math inline">\(X\)</span>. This tells us that <span class="math inline">\(X_{n_k}\)</span> converges almost surely, and, hence, in probability to <span class="math inline">\(X\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>, from which the conclusion follows via the triangle inequality, <span class="math display">\[
|X_n - X| \leq |X_n - X_{n_k}| + |X_{n_{k}}-X|,
\]</span> and Theorem 5.11.1 of <span class="citation">Gut (2014)</span>.</p>
</div>
<div id="when-cauchy-convergence-implies-mean-convergence" class="section level3">
<h3><span class="header-section-number">2.6.3</span> When Cauchy convergence implies Mean Convergence</h3>
<p>Given <span class="math inline">\(\{X_n, n\geq 1 \}\)</span>, we thus assume that the sequence <span class="math inline">\(\{ E|X_n|^{r}, n\geq 1 \}\)</span> is Cauchy convergent for some <span class="math inline">\(r&gt;0\)</span>. By Markov’s inequality, <span class="math display">\[
P(|X_m - X_n|&gt; \epsilon) \leq \frac{E|X_n - X_m|^{r}}{\varepsilon^r} \rightarrow 0 \text{ as } n \rightarrow \infty,
\]</span> that is, <span class="math inline">\(\{X_n , n \geq 1\}\)</span> is Cauchy convergent in probability, so that there exists a limiting random variable, <span class="math inline">\(X\)</span>, such that <span class="math inline">\(X_n \stackrel{p}{\rightarrow} X\)</span> as <span class="math inline">\(n\rightarrow\infty\)</span>.</p>
<p>Moreover, by Theorem 5.3.4, there exists a subsequence, <span class="math inline">\(\{X_{n_k}, k\geq 1\}\)</span>, such that <span class="math display">\[
X_{n_k} \stackrel{\text{a.s.}}{\rightarrow} X \text{ as } n\rightarrow \infty,
\]</span> so that, for <span class="math inline">\(m\)</span> fixed, <span class="math display">\[
X_m - X_{n_k} \stackrel{\text{a.s.}}{\rightarrow} X_{m} - X \text{ as } k \rightarrow\infty.
\]</span> An application of Fatou’s lemma then shows that <span class="math display">\[
E|X_{m} - X|^{r} \leq \text{lim inf}_{k \rightarrow \infty} E|X_{m} - X_{n_k}|^{r},
\]</span> from which it follows that <span class="math display">\[
\lim_{m \rightarrow \infty} E|X_m - X|^{r} \leq \lim_{m\rightarrow \infty} \text{lim inf}_{k\rightarrow \infty} E|X_m - X_{n_k}|^{r} = 0.
\]</span> This proves that <span class="math inline">\(X_n \stackrel{r}{\rightarrow} X\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
<p>Note that we thereby showa that the <span class="math inline">\(L^{p}\)</span>-space equipped with the norm <span class="math inline">\(\| \cdot \|_{p}\)</span> are complete for <span class="math inline">\(p \geq 1\)</span>.</p>
</div>
<div id="when-cauchy-convergence-implies-distributional-convergence" class="section level3">
<h3><span class="header-section-number">2.6.4</span> When Cauchy convergence implies Distributional Convergence</h3>
<p>This case is immediate, since distribution functions are real valued; if <span class="math display">\[
F_n(x) - F_{m}(x) \rightarrow 0 \text{ as } n \rightarrow \infty,
\]</span> then there exists a limiting value, <span class="math inline">\(F(x)\)</span>, say, such that <span class="math inline">\(F_n(x) \rightarrow F(x)\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
</div>
</div>
<div id="weak-laws-of-large-numbers" class="section level2">
<h2><span class="header-section-number">2.7</span> Weak Laws of Large Numbers</h2>
<p>We will prove several “weak laws of large numbers.” The first order of business is to define the mode of convergence that appears in the conclusions of the theorems.</p>
<div id="l2-weak-laws" class="section level3">
<h3><span class="header-section-number">2.7.1</span> L2 Weak Laws</h3>
<div class="theorem" btit="$L^2$ weak law">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be uncorrelated random variables with <span class="math inline">\(EX_i = \mu\)</span> and <span class="math inline">\(\text{var}(X_i) \leq C &lt; \infty\)</span>. If <span class="math inline">\(S_n = X_1 + \cdots + X_n\)</span>, then as <span class="math inline">\(n\rightarrow \infty\)</span>, <span class="math inline">\(\frac{S_n}{n} \rightarrow \mu\)</span> in <span class="math inline">\(L^2\)</span> and in probability.</p>
</div>
</div>
<div id="triangular-arrays" class="section level3">
<h3><span class="header-section-number">2.7.2</span> Triangular Arrays</h3>
<p>Many classical limit theorems in probability concern arrays <span class="math inline">\(X_{n,k}, 1\leq k \leq n\)</span> of random variables and investigate the limiting behavior of their row sums <span class="math inline">\(S_n = X_{n,1} + \cdots + X_{n,n}\)</span>. In most cases, we assume that the random variables on each row are independent, but for the next trivial (but useful) result we do not need that assumption. Indeed, here Sn can be any sequence of random variables.</p>
<div class="theorem" btit="$L^2$ weak law">
<p>Let <span class="math inline">\(\mu_n = ES_n\)</span>, <span class="math inline">\(\sigma_n^2 = \text{var}(S_n)\)</span>. If <span class="math inline">\(\frac{\sigma_{n}^2}{b_n^2} \rightarrow 0\)</span>, then <span class="math display">\[
\frac{S_n - \mu_n}{b_n } \stackrel{P}{\rightarrow}0.
\]</span></p>
</div>
</div>
<div id="truncation" class="section level3">
<h3><span class="header-section-number">2.7.3</span> Truncation</h3>
<p>To truncate a random variable <span class="math inline">\(X\)</span> at level <span class="math inline">\(M\)</span> means considering <span class="math display">\[
\bar{X} = X1_{(|X|)\leq M} = 
\begin{cases}
X &amp; \text{if } |X| \leq M\\
0 &amp; \text{if } |X| &gt; 0.
\end{cases}
\]</span></p>
<p>To extend the weak law to random variables without a finite second moment, we will truncate and then use Chebyshev’s inequality.</p>
<div class="theorem" btit="weak law for triangular arrays">
<p>For each <span class="math inline">\(n\)</span>, let <span class="math inline">\(X_{n,k}, 1\leq k \leq n\)</span>, be independent. Let <span class="math inline">\(b_n &gt; 0\)</span> with <span class="math inline">\(b_n \rightarrow \infty\)</span>, and let <span class="math inline">\(\bar{X}_{n,k} = X_{n,k}1_{(|X_{n,k}|\leq b_n)}\)</span>. Suppose that as <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
<p>(i). <span class="math inline">\(\sum_{k=1}^{n} P(|X_{n,k}|&gt;b_n) \rightarrow 0\)</span>, and</p>
<p>(ii). <span class="math inline">\(\frac{1}{b_n^2}\sum_{k=1}^{n}E\bar{X}_{n,k}^2 \rightarrow 0\)</span>.</p>
<p>If we let <span class="math inline">\(S_n = X_{n,1} + \cdots + X_{n,n}\)</span> and put <span class="math inline">\(a_n = \sum_{k=1}^{n}E\bar{X}_{n,k}\)</span>, then <span class="math display">\[
\frac{S_n - a_n}{b_n} \stackrel{P}{\rightarrow} 0.
\]</span></p>
</div>
<div class="theorem" btit="weak law of large numbers for a single sequence">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be i.i.d. with <span class="math display">\[
xP(|X_i|&gt;x)\rightarrow 0 \quad{\text{ as }x \rightarrow \infty.}
\]</span> Let <span class="math inline">\(S_n = X_1 + \cdots + X_n\)</span> and let <span class="math inline">\(\mu_{n} = E(X_{1}1_{(|X_1| \leq n)})\)</span>. Then <span class="math display">\[
\frac{S_n}{n}\stackrel{P}{\rightarrow}0.
\]</span></p>
</div>
<p>This theorem is the weak law in its most familiar form.</p>
<div class="theorem" btit="weak law of large numbers">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be i.i.d. with <span class="math inline">\(E|X_i| &lt; \infty\)</span>. Let <span class="math inline">\(S_n = X_1 + \cdots + X_n\)</span> and let <span class="math inline">\(\mu = EX_1\)</span>. Then Then <span class="math display">\[
\frac{S_n}{n}\stackrel{P}{\rightarrow}\mu.
\]</span></p>
</div>
<div class="example" btit="weak law does not hold">
<p>For an example where the weak law does not hold, suppose <span class="math inline">\(X_1, X_2, \ldots\)</span> are indpendent and have a <strong>Cauchy distribution</strong>: <span class="math display">\[
P(X_i \leq x) = \int_{-\infty}^x \frac{dt}{\pi(1+t^2)}.
\]</span> As <span class="math inline">\(x\rightarrow \infty\)</span>, <span class="math display">\[
P(|X_1| &gt; x)  = 2 \int_{x}^{\infty} \frac{dt}{\pi (1+t^2)} \sim \frac{2}{\pi}\int_{x}^{\infty}\frac{1}{t^2}dt = \frac{2}{\pi}\frac{1}{x}.
\]</span> From the necessity of this condition, we can conclude that there is no sequence of constants <span class="math inline">\(\mu_n\)</span> so that <span class="math inline">\(\frac{S_n}{n} -\mu_n \rightarrow 0\)</span>.</p>
</div>
</div>
</div>
<div id="borel-cantelli-lemmas" class="section level2">
<h2><span class="header-section-number">2.8</span> Borel-Cantelli Lemmas</h2>
<p>If An is a sequence of subsets of <span class="math inline">\(\Omega\)</span>, we let <span class="math display">\[
\text{lim sup}A_n = \lim_{m\rightarrow \infty}\cup_{n=m}^{\infty}A_n = \{ \omega \text{ that are in infinitely many } A_n \}
\]</span> (the limit exists since the sequence is decreasing in <span class="math inline">\(m\)</span>) and let <span class="math display">\[
\text{lim inf}A_n = \lim_{m\rightarrow \infty}\cap_{n=m}^{\infty}A_n = \{ \omega \text{ that are all but finitely many } A_n \}
\]</span> (the limit exists since the sequence is increasing in <span class="math inline">\(m\)</span>). The names <span class="math inline">\(\text{lim sup}\)</span> and <span class="math inline">\(\text{lim inf}\)</span> can be explained by nothing that <span class="math display">\[
\text{lim sup}_{n\rightarrow \infty} 1_{A_n} = 1_{(\text{lim sup }A_n)} \quad{\text{lim inf}_{n\rightarrow \infty} 1_{A_n} = 1_{(\text{lim inf }A_n)}}.
\]</span></p>
<p>It is common to write <span class="math inline">\(\text{lim sup}A_n = \{\omega: \omega \in A_n \text{i.o.}\}\)</span>, where i.o. stands for infinitely often. An example that illustrates the use of this notation is: “<span class="math inline">\(X_n\rightarrow 0\)</span> a.s. if and only if for all <span class="math inline">\(\epsilon &gt; 0, P(|X_n | &gt; \epsilon \text{ i.o.}) = 0\)</span>.”</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-46">Question</a>
</h4>
</div>
<div id="unnamed-chunk-46" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
1999-Summer
</div>
<div class="snu">
2003-Summer
</div>
<div class="snu">
2005-Winter
</div>
<div class="snu">
2005-Summer
</div>
<div class="snu">
2007-Summer
</div>
<div class="snu">
2008-Summer
</div>
<div class="snu">
2009-Summer
</div>
<div class="snu">
2009-Winter
</div>
<div class="snu">
2012-Winter
</div>
State Borel-Cantelli’s lemma.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-47">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-47" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Durrett (2019)</span>, Theorem 2.3.1)</p>
<div class="lemma" btit="Borel-Cantelli lemma">
<p>If <span class="math inline">\(\sum_{n=1}^{\infty} P(A_n) &lt; \infty\)</span>, then <span class="math display">\[
P(A_n \text{ i.o.})  =0.
\]</span></p>
</div>
</div>
</div>
</div>
</div>
<p>The next result is a typical application of the Borel-Cantelli lemma.</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-48">Question</a>
</h4>
</div>
<div id="unnamed-chunk-48" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2009-Summer
</div>
<div class="snu">
2009-Winter
</div>
<div class="theorem" btit="subsequence theoerm">
<p>(<span class="citation">Durrett (2019)</span>, Theorem 2.3.2)</p>
<p><span class="math inline">\(X_n \rightarrow X\)</span> in probability if and only if for every subsequence <span class="math inline">\(X_{n(m)}\)</span> there is a further subsequence <span class="math inline">\(X_{n(m_k)}\)</span> that converges almost surely to <span class="math inline">\(X\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class = "panel" style = "background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-49">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-49" class="panel-collapse collapse">
<div class="panel-body" style = "color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Durrett (2019)</span>, Theorem 2.3.2, page 59)</p>
<ul>
<li><p>Let <span class="math inline">\(\epsilon_k\)</span> be a sequence of positive numbers that <span class="math inline">\(\downarrow 0\)</span>. For each <span class="math inline">\(k\)</span>, there is an <span class="math inline">\(n(m_k) &gt; n (m_{k-1})\)</span> so that <span class="math inline">\(P(|X_{n(m_k)} - X| &gt; \epsilon_{k} \text{ i.o.}) = 0\)</span>, i.e., <span class="math inline">\(X_{n(m_k)} \rightarrow X\)</span> a.s.</p></li>
<li><p>To prove the second conclusion, we note that if for every subsequence <span class="math inline">\(X_{n(m)}\)</span> there is a further subsequence <span class="math inline">\(X_{n(m_k)}\)</span> that converges almost surely to <span class="math inline">\(X\)</span>, then we can apply the next lemmat to the sequence of numbers <span class="math inline">\(y_n = P(|X_n - X| &gt; \delta)\)</span> for any <span class="math inline">\(\delta &gt;0\)</span> to get the desired result.</p>
</div>
</div>
</div>
</div></li>
</ul>
<div id="Theorem 2.3.3 of Durret, 2019" class="theorem">
<blockquote>
<p>Let <span class="math inline">\(y_n\)</span> be a sequence of elements of a topological space. If every subsequence <span class="math inline">\(y_{n(m)}\)</span> has a further subsequence <span class="math inline">\(y_{n(m_k)}\)</span> that converges to <span class="math inline">\(y\)</span>, then <span class="math inline">\(y_n \rightarrow y\)</span>.</p>
</blockquote>
</div>
<p>Subsequence theorem allows us to upgrade convergence in probability to convergence almost surely.</p>
<div class="theorem">
<p>(<span class="citation">Jacod and Protter (2004)</span>, Theorem 17.5. (a)) If <span class="math inline">\(f\)</span> is continuous and <span class="math inline">\(X_n \rightarrow X\)</span> a.s., then <span class="math inline">\(\lim_{n\rightarrow\infty} f(X_n) = f(X)\)</span> a.s.</p>
<p>(<span class="citation">Durrett (2019)</span>, Theorem 2.3.4)</p>
<p>If <span class="math inline">\(f\)</span> is continuous and <span class="math inline">\(X_n \rightarrow X\)</span> in probability, then <span class="math inline">\(f(X_n) \rightarrow f(X)\)</span> in probability. If, in addition, <span class="math inline">\(f\)</span> is bounded, then <span class="math inline">\(Ef(X_n) \rightarrow Ef(X)\)</span>.</p>
</div>
<div id="Theorem 2.3.5 of Durret, 2019" class="theorem">
<blockquote>

</blockquote>
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be i.i.d. with <span class="math inline">\(EX_i =\mu\)</span> and <span class="math inline">\(EX_i^4 &lt;\infty\)</span>. If <span class="math inline">\(S_n = X_1 + \cdots + X_n\)</span>, then <span class="math inline">\(\frac{S_n}{n} \rightarrow \mu\)</span> a.s.</p>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-50">Question</a>
</h4>
</div>
<div id="unnamed-chunk-50" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2014-Summer
</div>
<div class="snu">
2014-Winter
</div>
<div class="kaist">
2017-Summer
</div>
Let <span class="math inline">\(A_n\)</span> be a sequence of independent events with <span class="math inline">\(P(A_n) &lt;1\)</span> for all <span class="math inline">\(n\)</span>. Show that <span class="math inline">\(P(\cup A_n) =1\)</span> implies <span class="math inline">\(\sum_{n} P(A_n) = \infty\)</span> and hence <span class="math inline">\(P(A_n \text{i.o. })=1\)</span>.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-51">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-51" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Durrett (2019)</span>, Exercise 2.3.8, page 64)</p>
<p>Pick <span class="math inline">\(\epsilon_n \downarrow 0\)</span> and pick <span class="math inline">\(c_n\)</span> so that <span class="math inline">\(P(|X_n | &gt; \epsilon_n c_n) \leq 2^{-n}\)</span>. Since <span class="math inline">\(\sum_n 2^{-n} &lt; \infty\)</span>, the Borel-Cantelli lemma implies <span class="math inline">\(P(|\frac{X_n}{c_n}|&gt;\epsilon_n \text{ i.o. }) =0\)</span>.</p>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-52">Question</a>
</h4>
</div>
<div id="unnamed-chunk-52" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2005-Winter
</div>
<div class="snu">
2009-Summer
</div>
<div class="snu">
2010-Summer
</div>
<div class="snu">
2012-Winter
</div>
<div class="snu">
2015-Winter
</div>
<div class="snu">
2016-Winter
</div>
<div class="snu">
2018-Summer
</div>
State and prove 2nd Borel-Cantelli’s lemma.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-53">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-53" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(<span class="citation">Durrett (2019)</span>, Theorem 2.3.7)</p>
<div class="lemma" btit="second Borel-Cantelli lemma">
<p>If the events <span class="math inline">\(A_n\)</span> are <em>independent</em>, then <span class="math inline">\(\sum P(A_n) = \infty\)</span> implies <span class="math inline">\(P(A_n \text{ i.o.})=1\)</span>.</p>
</div>
<div class="proof">
<p>Let <span class="math inline">\(M &lt; N &lt; \infty\)</span>. Independence and <span class="math inline">\(1-x \leq e^{-x}\)</span> imply <span class="math display">\[\begin{align}
P\Big( \cap_{n=M}^{N}A_n^{c}  \Big) &amp;= \prod_{n=M}^{N}(1-P(A_n)) \leq \prod_{n=M}^{N}\exp(-P(A_n))\\
&amp;= \exp \Bigg( -\sum_{n=M}^{N}P(A_n) \Bigg) \rightarrow 0 \text{ as } N \rightarrow 0.
\end{align}\]</span> So <span class="math inline">\(P(\cup_{n=M}^{\infty}A_n)=1\)</span> for all <span class="math inline">\(M\)</span>, and since <span class="math inline">\(\cup_{n=M}^{\infty}A_n \downarrow \text{lim sup}A_n\)</span> it follows that <span class="math inline">\(P(\text{lim sup} A_n)=1\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<p>A typical application of the second Borel-Cantelli lemma is:</p>
<div id="Theorem 2.3.8 of Durret, 2019" class="theorem">
<blockquote>

</blockquote>
<p>If <span class="math inline">\(X_1, X_2, \ldots\)</span> are i.i.d. with <span class="math inline">\(E|X_i| = \infty\)</span>, then <span class="math inline">\(P(|X_n| \geq n \text{ i.o. }) = 1\)</span>. So if <span class="math inline">\(S_n = X_1 + \cdots + X_n\)</span>, then <span class="math inline">\(P(\lim\frac{S_n}{n} \text{ exists }\in (-\infty, \infty))=0\)</span>.</p>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-54">Question</a>
</h4>
</div>
<div id="unnamed-chunk-54" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2018-Summer
</div>
Suppose that <span class="math inline">\(\{X_n\}\)</span> is an i.i.d. sequence with <span class="math inline">\(EX_{1}^{+}=\infty\)</span>. Show that <span class="math inline">\(\lim\sup_{n}\frac{|S_n|}{n} = \infty\)</span> a.s., where <span class="math inline">\(S_n = X_1 + \ldots X_n\)</span>.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-55">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-55" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(Variation of <span class="citation">Durrett (2019)</span>, Theorem 2.3.8)</p>
</div>
</div>
</div>
</div>
<div id="Theorem 2.3.9 of Durret, 2019" class="theorem">
<p>If <span class="math inline">\(A_1,A_2 ,\ldots\)</span> are pairwise independent and <span class="math inline">\(\sum_{n=1}^{\infty}P(A_n) = \infty\)</span>, then as <span class="math inline">\(n\rightarrow \infty\)</span> <span class="math display">\[
\sum_{m=1}^{n}1_{A_n}/\sum_{m=1}^{n}P(A_m) \stackrel{\text{a.s.}}{\rightarrow} 1.
\]</span></p>
</div>
</div>
<div id="strong-law-of-large-numbers" class="section level2">
<h2><span class="header-section-number">2.9</span> Strong Law of Large Numbers</h2>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-56">Question</a>
</h4>
</div>
<div id="unnamed-chunk-56" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2002-Summer
</div>
<p>State and prove</p>
<div class="theorem" btit="strong law of large numbers">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be pairwise independent identically distributed random variables with <span class="math inline">\(E|X_i|&lt; \infty\)</span>. Let <span class="math inline">\(EX_i = \mu\)</span> and <span class="math inline">\(S_n =X_1 + \cdots + X_n\)</span>. Then <span class="math inline">\(\frac{S_n}{n}\rightarrow \mu\)</span> a.s. as <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-57">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-57" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>As in the proof of the weak law of large numbers, we begin by truncating.</p>
<div class="lemma" btit="Lemma 2.4.2 of Durret 2019">
<p>Let <span class="math inline">\(Y_k = X_k 1_{(|X_k| \leq k)}\)</span> and <span class="math inline">\(T_n = Y_1 + \cdots + Y_n\)</span>. It is sufficient to prove that <span class="math inline">\(\frac{T_n}{n}\rightarrow \mu\)</span> a.s.</p>
</div>
<div class="proof">
<p><span class="math inline">\(\sum_{k=1}^{\infty} P(|X_k | &gt; k) \leq \int_{0}^{\infty}P(|X_1|&gt;t)dt = E|X_1|&lt; \infty\)</span> so <span class="math inline">\(P(X_k \neq Y_k \text{ i.o. }) = 0\)</span>. This shows that <span class="math inline">\(|S_n (\omega) - T_n (\omega)| \leq R(\omega) &lt; \infty\)</span> a.s. for all <span class="math inline">\(n\)</span>, from which the desired result follows.</p>
</div>
<p>The second step is not so intuitive, but it is an important part of this proof.</p>
<div class="lemma" btit="Lemma 2.4.3 of Durret 2019">
<p><span class="math inline">\(\sum_{k=1}^{\infty} \frac{\text{var}(Y_k)}{k^2}\leq 4 E|X_1| &lt;\infty\)</span>.</p>
</div>
<div class="proof">
<p>To bound the sum, we observe <span class="math display">\[
\text{var}(Y_k) \leq E(Y_k^2) = \int_{0}^{\infty} 2yP(|Y_k|&gt;y)dy \leq \int_{0}^{k}2yP(|X_1|&gt;y)dy
\]</span> so using Fubini’t theorem (since everything is <span class="math inline">\(\geq 0\)</span> and the sum is just an integral with respect to counting measure on <span class="math inline">\(\{1,2,\ldots \}\)</span>) <span class="math display">\[\begin{align}
\sum_{k=1}^{\infty}\frac{E(Y_k^2)}{k^2} &amp;\leq \sum_{k=1}^{\infty}\frac{1}{k^2}\int_{0}^{\infty} 1_{(y&lt;k)}2yP(|X_1|&gt;y)dy\\
&amp;= \int_{0}^{\infty}\Big\{ \sum_{k=1}^{\infty}\frac{1}{k^2} 1_{(y&lt;k)} \Big\} 2y P(|X_1|&gt; y)dy.
\end{align}\]</span> Since <span class="math inline">\(E|X_1| = \infty_{0}^{\infty}P(|X_1|&gt;y)dy\)</span>, we can complete the proof by showing:</p>
</div>
<div class="lemma" btit="Lemma 2.4.4 of Durret 2019">
<p>If <span class="math inline">\(y\geq 0\)</span>, then <span class="math inline">\(2y \sum_{k&gt;y}\frac{1}{k^2}\leq 4\)</span>.</p>
</div>
<div class="proof">
<p>We begin with the observation that if <span class="math inline">\(m \geq 2\)</span>, then <span class="math display">\[
\sum_{k\geq m} \frac{1}{k^2} \leq \int_{m-1}^{\infty}\frac{1}{x^2}dx = \frac{1}{m-1}.
\]</span> When <span class="math inline">\(y \geq 1\)</span>, the sum starts with <span class="math inline">\(k=[y]+1\geq 2\)</span>, so <span class="math display">\[
2y \sum_{k&gt;y}\frac{1}{k^2} \leq \frac{2y}{[y]}\leq 4
\]</span> since <span class="math inline">\(\frac{2y}{[y]}\leq 2\)</span> for <span class="math inline">\(y\geq 1\)</span> (the worst case being <span class="math inline">\(y\)</span> close to <span class="math inline">\(2\)</span>). To cover <span class="math inline">\(0\leq y &lt; 1\)</span>, we note that in this case <span class="math display">\[
2y \sum_{k&gt;y}\frac{1}{k^2}\leq 2 \Big( 1 + \sum_{k=2}^{\infty}\frac{1}{k^2}  \Big) \leq 4.
\]</span> This establishes Lemma 2.4.4, which completes the proof of Lemma 2.4.3 and of the theorem.</p>
</div>
The first two steps, Lemmas 2.4.2 and 2.4.3, are standard. Etemadi’s inspiration was that since <span class="math inline">\(X_n^{+}, n \geq 1\)</span>, and <span class="math inline">\(X_n^{-},n\geq 1\)</span> satisfy the assumptions of the theorem and <span class="math inline">\(X_n = X_n^{+} - X_n^{-}\)</span>, we can without loss of generality suppose <span class="math inline">\(X_n \geq 0\)</span>. As in the proof of Theorem 2.3.9 of <span class="citation">Durrett (2019)</span>, we will prove the result first for a subsequence and then use monotonicity to control the values in between. This time, however, we let <span class="math inline">\(\alpha &gt; 1\)</span> and <span class="math inline">\(k(n) = [\alpha^n]\)</span>. Chebyshev’s inequality implies that if <span class="math inline">\(\epsilon &gt;0\)</span> <span class="math display">\[\begin{align}
&amp;\sum_{n=1}^{\infty}P(|T_{k(n)}- ET_{k(n)}| &gt; \epsilon k(n)) \leq \frac{1}{\epsilon^2} \sum_{n=1}^{\infty}\frac{\text{var}(T_{k(n)})}{k(n)^2}\\
&amp;\frac{1}{\epsilon^2} \sum_{n=1}^{\infty}\frac{1}{k(n)^2} \sum_{m=1}^{k(n)}\text{var}(Y_m) = \frac{1}{\epsilon^2}\sum_{m=1}^{\infty}\text{var}(Y_m) \sum_{n:k(n)\geq m}\frac{1}{k(n)^2}
\end{align}\]</span> where we have used Fubini’s theorem to interchange the two summations of nonnegative terms. Now <span class="math inline">\(k(n) = [\alpha^n]\)</span> and <span class="math inline">\([\alpha^n]\geq \frac{\alpha^n}{2}\)</span> for <span class="math inline">\(n\geq 1\)</span>, so summing the geometric series and noting that the first term is <span class="math inline">\(\leq \frac{1}{m^2}\)</span>: <span class="math display">\[
\sum_{n:\alpha^n \geq m}\frac{1}{[\alpha^n]^2} \leq 4 \sum_{n:\alpha^n \geq m}\alpha^{-2n} \leq 4(1-\alpha^2)^{-1}m^{-2}.
\]</span> Combining our computation shows <span class="math display">\[
\sum_{n=1}^{\infty}P(|T_{k(n)}- ET_{k(n)}| &gt; \epsilon k(n))\leq 4(1-\alpha^2)^-1 \epsilon^{-2}\sum_{m=1}^{\infty}E(Y_m^2)m^{-2}&lt;\infty
\]</span> by Lemma 2.4.3 of <span class="citation">Durrett (2019)</span>. Since <span class="math inline">\(\epsilon\)</span> is arbitrary <span class="math inline">\((T_{k(n)} - ET_{k(n)})/k(n)\rightarrow 0\)</span> a.s. The dominated convergence theorem implies <span class="math inline">\(EY_k \rightarrow EX_1\)</span> as <span class="math inline">\(k\rightarrow \infty\)</span>, so <span class="math inline">\(ET_{k(n)} / k(n) \rightarrow EX_1\)</span> and we have shown <span class="math inline">\(T_{k(n)}/k(n)\rightarrow EX_1\)</span> a.s. To handle the intermediate values, we observe that if <span class="math inline">\(k(n) \leq m &lt; k(n+1)\)</span> <span class="math display">\[
\frac{T_{k(n)}}{k(n+1)}\leq \frac{T_m}{m}\leq \frac{T_{k(n+1)}}{k(n)}
\]</span> (here we use <span class="math inline">\(Y_i \geq 0\)</span>), so recalling <span class="math inline">\(k(n) = [\alpha^n]\)</span>, we have <span class="math inline">\(k(n+1)/k(n) \rightarrow \alpha\)</span> and <span class="math display">\[
\frac{1}{\alpha}EX_1 \leq \text{lim inf}_{n\rightarrow \infty}T_{m}/m
\leq \text{lim sup}_{n\rightarrow \infty}T_{m}/m \leq \alpha EX_1.\]</span> Since <span class="math inline">\(\alpha &gt; 1\)</span> arbitrary, the proof is complete.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class = "panel" style = "background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-58">Question</a>
</h4>
</div>
<div id="unnamed-chunk-58" class="panel-collapse collapse in">
<div class="panel-body" style = "color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2007-Summer
</div>
<div class="snu">
2013-Summer
</div>
<div class="snu">
2016-Winter
</div>
<div class="snu">
2017-Summer
</div>
<p>Prove</p>
<ul>
<li><p>Let <span class="math inline">\(X_n\)</span> be i.i.d. r.v.s with the finite first moment. Prove that <span class="math inline">\(Y_n = \max_{1,\ldots, n} \frac{X_i}{n}\)</span> converges to <span class="math inline">\(0\)</span> almost surely. (2007 Summer, 2017 Summer)</p></li>
<li><p>Let <span class="math inline">\(X_n\)</span> be i.i.d. r.v.s having the same common distribution with finite variance. Show that <span class="math inline">\(\max_{1\leq k \leq n}\frac{|X_k|}{\sqrt{n}}\rightarrow 0\)</span> in probability. (2016 Winter)</p></li>
<li><p>Let <span class="math inline">\(X_n\)</span> be i.i.d. r.v.s with <span class="math inline">\(EX_{1}^{4} &lt; \infty\)</span>. Show that <span class="math inline">\(\max_{1\leq k \leq n} \frac{|X_k|}{n^{1/4}}\rightarrow 0\)</span> a.s. (2013 Summer)</p>
</div>
</div>
</div>
</div></li>
</ul>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-59">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-59" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>Related to Exercise 2.3.17 of <span class="citation">Durrett (2019)</span>.</p>
<p>(Page 163, Theorem 4.2 (8) of <span class="citation">Shorack (2017)</span>)</p>
<p><span class="citation">Shorack (2017)</span> showed that when <span class="math inline">\(X, X_1, X_2, \ldots\)</span> be i.i.d. r.v.s. Then: <span class="math display">\[
E|X| &lt; \infty \Leftrightarrow M_n \equiv [\frac{1}{n}\max_{1\leq k \leq n}|X_k|]\stackrel{\text{a.s}}{\rightarrow}0 \Leftrightarrow \frac{X_n}{X}\stackrel{\text{a.s}}{\rightarrow}0 \Leftrightarrow M_n \stackrel{\mathcal{L}_{1}}{\rightarrow} 0.
\]</span></p>
<p>This condition shows the sence in which SLLN is tied to the size of the maximal summand.</p>
<p>(Page 166 of <span class="citation">Shorack (2017)</span>)</p>
<p>Suppose <span class="math inline">\(M_n \stackrel{\text{a.s.}}{\rightarrow}0\)</span>. Then a.s. for all <span class="math inline">\(n \geq\)</span> (some <span class="math inline">\(n_\omega)\)</span> we have <span class="math display">\[
\frac{\max_{1\leq k \leq n}|X_k|}{n} &lt; \epsilon,
\]</span> and hence <span class="math inline">\(\frac{|X_n|}{n} &lt; \epsilon\)</span>. We merely repeat this last statement, writing <span class="math display">\[
A_n \equiv [\frac{|X_n|}{n}\geq \epsilon]
\]</span> satisfies <span class="math inline">\(P(A_n \text{ i.o.})=P(\frac{|X_n|}{n}\geq \epsilon)&lt;\infty\)</span>.</p>
<p>Conversely, suppose <span class="math inline">\(E|X|&lt;\infty\)</span>. Then <span class="math inline">\(\frac{S_n}{n}\stackrel{\text{a.s.}}{\rightarrow}\mu\)</span> by the SLLN. Since <span class="math display">\[
\frac{X_n}{n}= \frac{X_n-n\mu}{n}-\frac{n-1}{n}[\frac{S_{n-1}-(n-1)\mu}{n-1}] +\frac{\mu}{n}\stackrel{\text{a.s.}}{\rightarrow} 0 - 1\cdot 0 + 0 = 0,
\]</span> we have a.s. that <span class="math display">\[
\frac{|X_n|}{n}\leq \epsilon,  \forall n \geq (\text{some }n_{\omega}).
\]</span> Thus for all <span class="math inline">\(n\)</span> exceeding some even larger <span class="math inline">\(n_{\omega}&#39;\)</span>, we have <span class="math display">\[
\begin{align}
\Big[\max_{1\leq k \leq n}\frac{|X_k|}{n}\Big]&amp;=\Big[\max_{1\leq k \leq n}\frac{k}{n}\cdots\frac{|X_k|}{k}\Big]\leq \Big[\max_{1\leq k \leq n_{\omega}}\frac{|X_k|}{n}\Big] \vee \Big[\max_{k \geq n_{\omega}}\frac{|X_k|}{k}\Big]\\
&amp;\leq \frac{1}{n}[\text{ a fixed number depending on }\omega] + \epsilon \leq 2\epsilon,
\end{align}
\]</span> where we will have to increase the specification on <span class="math inline">\(n_{\omega}&#39;\)</span> for the last inequality. Thus <span class="math inline">\(M_n \stackrel{\text{a.s.}}{\rightarrow}0\)</span>.</p>
<p>Finally, note that <span class="math inline">\(M_n \stackrel{\mathcal{L}_{1}}0\)</span> if and only if <span class="math inline">\(E|X|&lt;\infty\)</span> (exercise). From the fact that if <span class="math inline">\(X\geq 0\)</span> has a df F, then <span class="math display">\[
\int_{0}^{\infty}P(X&gt;x)dx = EX =\int_{0}^{\infty}(1-F(x))dx \text{ and }EX = \int_{0}^{1}F^{-1}dt.
\]</span> Using this equation, we see that <span class="math display">\[
E|X|&lt; \infty \Leftrightarrow \int_{0}^{\infty}P(|X|&gt;x)dx &lt; \infty \Leftrightarrow \int_{0}^{1}|F^{-1}(t)|dt &lt; \infty.
\]</span></p>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-60">Question</a>
</h4>
</div>
<div id="unnamed-chunk-60" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2003-Summer
</div>
<div class="snu">
2008-Summer
</div>
<div class="snu">
2014-Winter
</div>
<div class="snu">
2017-Winter
</div>
<div class="snu">
2018-Winter
</div>
Let <span class="math inline">\(X_n\)</span> be a sequence of i.i.d. random variables with <span class="math inline">\(E|X_n| &lt; \infty\)</span> and <span class="math inline">\(EX_n \neq 0\)</span>. Show that <span class="math display">\[
\frac{\max_{1\leq i \leq n}|X_i|}{|S_n|} \rightarrow 0 \text{ a.s.}
\]</span>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-61">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-61" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>(Exercise 7.16, <span class="citation">Resnick (2014)</span>)</p>
<p>Divide the upper and lower side by <span class="math inline">\(n\)</span>, and the lower side <span class="math display">\[
|\frac{S_n}{n}|\sim |\mu| \neq 0 \text{ a.s. }
\]</span> by the strong law of large numbers. Therefore, it suffices to show that <span class="math display">\[
\frac{M_n}{n}=\frac{\max_{i\leq n}|X_i|}{n}\stackrel{\text{a.s.}}{\rightarrow} 0,
\]</span> as <span class="math inline">\(n\rightarrow \infty\)</span>. Since <span class="math inline">\(E(|X_1|) &lt;\infty\)</span>, we have <span class="math display">\[
\lim_{n\rightarrow \infty}\frac{1}{n}|X_n(\omega)| =0,
\]</span> for <span class="math inline">\(\omega \in \Omega\)</span> and <span class="math inline">\(P(\Omega) =1\)</span>. Now for each <span class="math inline">\(n\)</span>, there exists <span class="math inline">\(k(n)\leq n\)</span> (which is random) such that <span class="math inline">\(M_n = X_{k(n)}\)</span> and therefore, <span class="math display">\[
\frac{M_n}{n}\leq \frac{|X_{k(n)}|}{k(n)}.
\]</span> Suppose <span class="math inline">\(\omega in \Omega\)</span>. There are two cases.</p>
<ol style="list-style-type: decimal">
<li><p>When <span class="math inline">\(k(n,\omega) \rightarrow \infty\)</span>, so that <span class="math display">\[
\frac{1}{n}M_{n}(\omega) \leq \frac{|X_{k(n,\omega)(\omega)}|}{k(n,\omega)}\rightarrow 0.
\]</span></p></li>
<li><p>For some integer <span class="math inline">\(M(\omega) &lt; \infty\)</span>, we have <span class="math inline">\(k(n,\omega) \leq M\)</span> so that <span class="math display">\[
\frac{1}{n}M_{n}(\omega) \leq \frac{1}{n}\bigvee_{i=1}^{M(\omega)}|X_{i}(\omega)| \rightarrow 0.
\]</span></p></li>
</ol>
<p>(From solution 7 or Probability Theory(MATH280A) course)</p>
<p>Alternatively, let <span class="math inline">\(\lambda_{n} = \varepsilon n\)</span>, and observe that <span class="math inline">\(\sum_{n}P(|X_n| &gt; \lambda_n) = \sum P(|X_i|/\epsilon &gt; n) \approx E(|X_{i}|/\varepsion) &lt; \infty\)</span>. By Borel-Cantelli Lemma, this implies <span class="math inline">\(P(|X_n|&gt; \lambda_n \text{ i.o.}) = 0\)</span>. It also implies <span class="math inline">\(P(\max_{i\leq n}&gt; \lambda_n \text{ i.o.})=0\)</span>. Therefore, <span class="math inline">\(\text{lim sup}_{n}[\max_{i\leq n}|X_i|/n]\leq \varepsilon\)</span> almost surely for arbitrary <span class="math inline">\(\varepsilon\)</span>, which is <span class="math inline">\(\max_{i\leq n} |X_i|/n \rightarro 0\)</span> with probability one.</p>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-62">Question</a>
</h4>
</div>
<div id="unnamed-chunk-62" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2011-Winter
</div>
Suppose that <span class="math inline">\(X_n\)</span> are i.i.d. r.v.’s with <span class="math inline">\(EX_1 = 0\)</span> and <span class="math inline">\(EX_{1}^{2} &lt;\infty\)</span> and let <span class="math inline">\(S_n = X_1 + \cdots +X_n\)</span>. Show that <span class="math inline">\(\max_{1\leq k \leq n} \frac{|X_k|}{S_n}\rightarrow 0\)</span> in probability and <span class="math inline">\(\max_{1\leq k \leq n}\frac{|X_k|}{S_n + nc} \rightarrow 0\)</span> a.s. for every <span class="math inline">\(c\neq 0\)</span>.
</div>
</div>
</div>
</div>
<p>The next result shows that the strong law holds whenever <span class="math inline">\(EX_i\)</span> exists.</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-63">Question</a>
</h4>
</div>
<div id="unnamed-chunk-63" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2009-Summer
</div>
<div class="snu">
2009-Winter
</div>
<div class="snu">
2010-Summer
</div>
<div class="theorem" btit="strong law holds whenever first moment exists">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be i.i.d. with <span class="math inline">\(EX_i^{+}= \infty\)</span> and <span class="math inline">\(EX_{i}^{-}&lt;\infty\)</span>. If <span class="math inline">\(S_n = X_1 + \cdots + X_n\)</span>, then <span class="math inline">\(\frac{S_n}{n}\rightarrow \infty\)</span> a.s.</p>
</div>
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-64">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-64" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<p>The main idea is to try truncations of <span class="math inline">\(X_{n}^{+}\)</span> and use the classical SLLN. <span class="citation">(Resnick 2014)</span></p>
<p>(Theorem 2.4.5 of <span class="citation">Durrett (2019)</span>)</p>
Let <span class="math inline">\(M&gt;0\)</span> and <span class="math inline">\(X_i^{M} = X_i \wedge M\)</span>. The <span class="math inline">\(X_i^M\)</span> are i.i.d. with <span class="math inline">\(E|X_i^{M}| &lt; \infty\)</span>, so if <span class="math inline">\(S_n^{M} = X_{1}^{M}+\cdots X_n^{M}\)</span>, then SLLN implies <span class="math inline">\(S_{n}^{M}/n\rightarrow EX_{i}^M\)</span>. SInce <span class="math inline">\(X_i \geq X_i^{M}\)</span>, it follows that <span class="math display">\[
\text{lim inf}_{n\rightarrow\infty}\frac{S_n}{n} \geq \lim_{n\rightarrow\infty}\frac{S_n^M}{n}=EX_i^{M}.
\]</span> The monoton convergence theorem implies <span class="math inline">\(E(X_i^M)^{+}\uparrow EX_i^{+}=\infty\)</span> as <span class="math inline">\(M\uparrow \infty\)</span>, so <span class="math inline">\(EX_i^{M} = E(X_i^{M})^{+} - E(X_i^{M})^{-} \uparrow \infty\)</span>, and we have <span class="math inline">\(\text{lim inf}_{n\rightarrow \infty} \frac{S_n}{n}\geq \infty\)</span>, which implies the desired result.
</div>
</div>
</div>
</div>
<p>The rest of this section is devoted to applications of the strong law of large numbers.</p>
<div class="theorem">
<p>If <span class="math inline">\(EX_ \mu \leq \infty\)</span>, then as <span class="math inline">\(t\rightarrow \infty\)</span>, <span class="math display">\[
\frac{N_t}{t}\rightarrow \frac{1}{\mu} \text{ a.s. } \quad{(\frac{1}{\infty} = 0).}
\]</span></p>
</div>
<div class="example" btit="Empricial distribution function">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be i.i.d. with distribution <span class="math inline">\(F\)</span> and let <span class="math display">\[
F(x) = \frac{1}{n}\sum_{m=1}^{n}1_{(X_m \leq x).}
\]</span> <span class="math inline">\(F_{n}(x)\)</span> = the observed frequency of values that are <span class="math inline">\(\leq x\)</span>, hence the name given here. The next tresult shows that <span class="math inline">\(F_n\)</span> converges uniformly to <span class="math inline">\(F\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-65">Question</a>
</h4>
</div>
<div id="unnamed-chunk-65" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2007-Summer
</div>
<div class="snu">
2012-Summer
</div>
<div class="snu">
2012-Winter
</div>
State Glivenko-Cantelli theorem.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-66">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-66" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="theorem" btit="the Glivenko-Cantelli theorem">
<p>As <span class="math inline">\(n\rightarrow \infty\)</span>, <span class="math display">\[
\sup_{x} |F_{n}(x) - F(x) | \rightarrow 0 \text{ a.s.}
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="convergence-of-random-series" class="section level2">
<h2><span class="header-section-number">2.10</span> Convergence of Random Series</h2>
<p>In this section, we will pursue a second approach to the strong law of large numbers based on the convergence of random series.</p>
<p>This approach has the advantage that it leads to</p>
<ul>
<li><p>estimates on the rate of convergence under moment assumptions,</p></li>
<li><p>a negative result for the infinite mean case</p></li>
</ul>
<p>To state the first result, we need some notation. Let <span class="math inline">\(\mathcal{F}_{n}&#39; = \sigma(X_n , X_{n+1},\ldots)=\)</span>the future after time <span class="math inline">\(n=\)</span>the smallest <span class="math inline">\(\sigma\)</span>-field with respect to which all the <span class="math inline">\(X_m, m\geq n\)</span> are measurable. Let</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-67">Question</a>
</h4>
</div>
<div id="unnamed-chunk-67" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2005-Summer
</div>
<div class="snu">
2008-Summer
</div>
<div class="snu">
2010-Summer
</div>
<div class="snu">
2012-Summer
</div>
State the tail <span class="math inline">\(\sigma\)</span>-field.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-68">Question</a>
</h4>
</div>
<div id="unnamed-chunk-68" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2005-Summer
</div>
<div class="snu">
2005-Winter
</div>
<div class="snu">
2007-Summer
</div>
<div class="snu">
2010-Summer
</div>
<div class="snu">
2012-Winter
</div>
State Kolmogorov’s 0-1 law.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-69">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-69" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="theorem" btit="Kolmogorov&#39;s 0-1 law">
<p>If <span class="math inline">\(X_1, X_2, \ldots\)</span> are independent and <span class="math inline">\(A\in \mathcal{T}\)</span>, then <span class="math inline">\(P(A)= 0\)</span> or <span class="math inline">\(1\)</span>.</p>
</div>
<div class="proof">
<p>We will show taht <span class="math inline">\(A\)</span> is independent of itself, that is, <span class="math inline">\(P(A\cap A) = P(A)P(A)\)</span>, so <span class="math inline">\(P(A) = P(A)^{2}\)</span>, and hence <span class="math inline">\(P(A)=0\)</span> or <span class="math inline">\(1\)</span>. We will sneak up on this conclusion in two steps:</p>
<p>(a). <span class="math inline">\(A \in \sigma(X_1, \ldots, X_k)\)</span> and <span class="math inline">\(B\in \sigma(X_{k+1}, X_{k+2}, \ldots)\)</span> are indepdnent.</p>
<p>If <span class="math inline">\(B\in\sigma(X_{k+1}, \ldots, X_{k+j})\)</span> for some <span class="math inline">\(j\)</span>, this follows from Theorem 2.1.9 Since <span class="math inline">\(\sigma(X_1, \ldots, X_k)\)</span> and <span class="math inline">\(\cup_{j}\sigma(X_{k+1},\ldots, X_{k+j})\)</span> are <span class="math inline">\(\pi\)</span>-systems that contain <span class="math inline">\(\Omega\)</span> (a) follows from Theorem 2.1.7 of <span class="citation">Durrett (2019)</span>.</p>
<p>(b). <span class="math inline">\(A\in \sigma(X_1, X_2, \ldots)\)</span> and <span class="math inline">\(B\in\mathcal{T}\)</span> are independent.</p>
<p>Since <span class="math inline">\(\mathcal{T} \subset \sigma(X_{k+1}, X_{k+2}, \ldots)\)</span>, if <span class="math inline">\(A \in \sigma(X_1, \ldots, X_k)\)</span> for some <span class="math inline">\(k\)</span>, this follows from (a). <span class="math inline">\(\cup_{k}\sigma(X_1, \ldots, X_k)\)</span> and <span class="math inline">\(\mathcal{T}\)</span> are <span class="math inline">\(\pi\)</span>-systems that contain <span class="math inline">\(\Omega\)</span>, so (b) follows from Theorem 2.1.7 of <span class="citation">Durrett (2019)</span>.</p>
<p>Since <span class="math inline">\(\mathcal{T} \subset \sigma(X_1, X_2, \ldots)\)</span>, (b) implies <span class="math inline">\(A\in\mathcal{T}\)</span> is independent of itself and Theorem 2.5.3 of <span class="citation">Durrett (2019)</span> follows.</p>
</div>
</div>
</div>
</div>
</div>
<div class="theorem" btit="Hewitt-Savege 0-1 law">
<p>If <span class="math inline">\(X_1, X_2, \ldots\)</span> are i.i.d. and <span class="math inline">\(A\in \mathcal{E}\)</span> then <span class="math inline">\(P(A) \in \{0,1 \}\)</span>.</p>
</div>
<p>The next result will help us prove the probability is 1 in certain situations.</p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-70">Question</a>
</h4>
</div>
<div id="unnamed-chunk-70" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2002-Summer
</div>
<div class="snu">
2008-Winter
</div>
<div class="snu">
2012-Winter
</div>
<div class="kaist">
2013-Winter
</div>
<div class="snu">
2015-Summer
</div>
<div class="kaist">
2017-Winter
</div>
<div class="kaist">
2018-Winter
</div>
<div class="kaist">
2019-Winter
</div>
State and prove Kolmogorov’s inequality.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-71">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-71" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="theorem" btit="Kolmogorov&#39;s maximal inequality">
<p>Suppose <span class="math inline">\(X_1, \ldots, X_n\)</span> are independent with <span class="math inline">\(EX_i = 0\)</span> and <span class="math inline">\(\text{var}(X_i) &lt;\infty\)</span>. If <span class="math inline">\(S_n = X_1 + \cdots + X_n\)</span>, then <span class="math display">\[
P(|S_n|\geq x) \leq \frac{1}{x^2}\text{var}(S_n).
\]</span></p>
</div>
<div class="proof">
<p>Let <span class="math inline">\(A_k = \{ |S_k| \geq x\)</span> but <span class="math inline">\(|S_j| &lt; x\)</span> for <span class="math inline">\(j&lt;k\}\)</span>, i.e., we break things down according to the time that <span class="math inline">\(|S_k|\)</span> first exceeds <span class="math inline">\(x\)</span>. Since the <span class="math inline">\(A_k\)</span> are disjoint and <span class="math inline">\((S_n - S_k)^{2}\geq 0\)</span>, <span class="math display">\[\begin{align}
ES_n^2 &amp;\geq \sum_{k=1}^{n}\int_{A_k}S_n^2 dP = \sum_{k=1}^n \int_{A_k} S_k^2 + 2S_k (S_n - S_k) + (S_n - S_k)^{2} dP\\
&amp;\geq \sum_{k=1}^{n}\int_{A_k}S_{k}^2 dP + \sum_{k=1}^{n}\int 2 S_k 1_{A_k} \cdot (S_n - S_k)dP
\end{align}\]</span> <span class="math inline">\(S_k 1_{A_k} \in \sigma(X_1, \ldots X_k)\)</span> and <span class="math inline">\(S_n - S_k \in \sigma(X_{k+1}, \ldots, X_n)\)</span> are independent by Theorem 2.1.10 of <span class="citation">Durrett (2019)</span> so using Theorem 2.1.13 of <span class="citation">Durrett (2019)</span> and <span class="math inline">\(E(S_n - S_k)=0\)</span> shows <span class="math display">\[
\int 2S_k 1_{A_k}\cdot(S_n - S_k)dP = E(2S_k 1_{A_k})\cdot E(S_n - S_k) = 0.
\]</span> Using now the fact that <span class="math inline">\(|S_k| \geq x\)</span> on <span class="math inline">\(A_k\)</span> and the <span class="math inline">\(A_k\)</span> are disjoint, <span class="math display">\[
ES_n^{2} \geq \sum_{k=1}^{n}\int_{A_k}S_{k}^2dP \geq \sum_{k=1}^{n} x^2 P(A_k) = x^2 P(\max_{1\leq k\leq n}|S_k| \geq x).
\]</span></p>
</div>
</div>
</div>
</div>
</div>
<p>We turn now to our results on convergence of series. To state them, we need a definition. We say that <span class="math inline">\(\sum_{n=1}^{\infty} a_n\)</span> converges if <span class="math inline">\(\lim_{N\rightarrow \infty}\sum_{n=1}^{N}a_{n}\)</span> exists.</p>
<div class="theorem" btit="Theorem 2.5.6 of Durrett 2019">
<p>Suppose <span class="math inline">\(X_1, X_2, \ldots\)</span> are independent and have <span class="math inline">\(EX_n = 0\)</span>. If <span class="math display">\[
\sum_{n=1}^{\infty}\text{var}(X_n)&lt;\infty
\]</span> then with probability one <span class="math inline">\(\sum_{n=1}^{\infty}X_{n}(\omega)\)</span> converges.</p>
</div>
<p>Theorem 2.5.6 is sufficient for all of our applications, but our treatment would not be complete if we did not mention the last word on convergence of random series. <span class="citation">(Durrett 2019)</span></p>
<div class="panel-group">
<div class="panel" style="background-color:rgba(135, 206, 235, 1); border:2px solid rgba(108, 165, 188, 1);">
<div class="panel-heading" style="background-color:rgba(108, 165, 188, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<i class="fa fa-star"></i><a class = "" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-72">Question</a>
</h4>
</div>
<div id="unnamed-chunk-72" class="panel-collapse collapse in">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="snu">
2003-Summer
</div>
<div class="snu">
2004-Winter
</div>
<div class="snu">
2005-Summer
</div>
<div class="snu">
2005-Winter
</div>
<div class="snu">
2008-Summer
</div>
<div class="snu">
2011-Summer
</div>
State Kolmogorov’s three-series theorem.
</div>
</div>
</div>
</div>
<div class="panel-group">
<div class="panel" style="background-color:rgba(255, 192, 203, 1); border:2px solid rgba(204, 154, 162, 1);">
<div class="panel-heading" style="background-color:rgba(204, 154, 162, 1); color:rgba(0, 0, 0, 1)!important;">
<h4 class="panel-title">
<a class = "collapsed" style="color: rgba(0, 0, 0, 1);" data-toggle="collapse" href="#unnamed-chunk-73">Solution</a>
</h4>
</div>
<div id="unnamed-chunk-73" class="panel-collapse collapse">
<div class="panel-body" style="color:rgba(0, 0, 0, 1)!important;">
<div class="theorem" btit="Kolmogorov&#39;s three-series theorem">
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be independent. Let <span class="math inline">\(A &gt; 0\)</span> and let <span class="math inline">\(Y_i = X_i 1_{(|X_i| \leq A)}\)</span>. In order that <span class="math inline">\(\sum_{n=1}^{\infty} X_n\)</span> converges a.s., it is necessary and sufficient that</p>
<p>(i). <span class="math inline">\(\sum_{n=1}^{\infty}P(|X_n|&gt;A)&lt;\infty\)</span>,</p>
<p>(ii). <span class="math inline">\(\sum_{n=1}^{\infty}EY_n\)</span> converges, and</p>
<p>(iii). <span class="math inline">\(\sum_{n=1}^{\infty}\text{var}(Y_n)&lt;\infty\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="theorem" btit="Kronecker&#39;s lemma">
<p>If <span class="math inline">\(a_n \uparrow \infty\)</span> and <span class="math inline">\(\sum_{n=1}^{\infty} \frac{x_n}{a_n}\)</span> converges, then <span class="math display">\[
\frac{1}{a_n}\sum_{m=1}^{n}x_m \rightarrow 0.
\]</span></p>
</div>
<div id="refs" class="references">
<div id="ref-Durrett2019">
<p>Durrett, Rick. 2019. <em>Probability: Theory and Examples</em>. 5th ed. Cambridge University Press. <a href="https://www.ebook.de/de/product/34699864/rick_duke_university_north_carolina_durrett_probability.html">https://www.ebook.de/de/product/34699864/rick_duke_university_north_carolina_durrett_probability.html</a>.</p>
</div>
<div id="ref-Gut2014">
<p>Gut, Allan. 2014. <em>Probability: A Graduate Course</em>. 2nd ed. Springer New York. <a href="https://www.ebook.de/de/product/23277813/allan_gut_probability_a_graduate_course.html">https://www.ebook.de/de/product/23277813/allan_gut_probability_a_graduate_course.html</a>.</p>
</div>
<div id="ref-Jacod2004">
<p>Jacod, Jean, and Philip Protter. 2004. <em>Probability Essentials</em>. 2nd ed. Springer Berlin Heidelberg. <a href="https://www.ebook.de/de/product/1968326/jean_jacod_philip_protter_probability_essentials.html">https://www.ebook.de/de/product/1968326/jean_jacod_philip_protter_probability_essentials.html</a>.</p>
</div>
<div id="ref-Resnick2014">
<p>Resnick, Sidney I. 2014. <em>A Probability Path</em>. Birkhäuser Boston. <a href="https://doi.org/10.1007/978-0-8176-8409-9">https://doi.org/10.1007/978-0-8176-8409-9</a>.</p>
</div>
<div id="ref-Shorack2017">
<p>Shorack, Galen R. 2017. <em>Probability for Statisticians</em>. 2nd ed. Springer International Publishing. <a href="https://www.ebook.de/de/product/28189528/galen_r_shorack_probability_for_statisticians.html">https://www.ebook.de/de/product/28189528/galen_r_shorack_probability_for_statisticians.html</a>.</p>
</div>
<div id="ref-Stoyanov2014">
<p>Stoyanov, Jordan M. 2014. <em>Counterexamples in Probability: Third Edition</em>. 3rd ed. DOVER PUBN INC. <a href="https://www.ebook.de/de/product/19654789/jordan_m_stoyanov_counterexamples_in_probability_third_edition.html">https://www.ebook.de/de/product/19654789/jordan_m_stoyanov_counterexamples_in_probability_third_edition.html</a>.</p>
</div>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = false;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
